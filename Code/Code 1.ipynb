{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZjmHveQ3RCG",
        "outputId": "3e890627-00d5-43de-f677-a790a735c1be"
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.5.4-py3-none-any.whl (524 kB)\n",
            "\u001b[K     |████████████████████████████████| 524 kB 11.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 42.7 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.42.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 51.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.8)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 54.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=5b21d626f9a55f2f0fc41a14ca586e35ad2a25c750100495444cd2db634d74ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.4 torchmetrics-0.6.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B34jnZQh2m8Y"
      },
      "source": [
        "import pandas as pd\n",
        "train_df_easy = pd.read_csv('/content/data_easy.csv')\n",
        "train_df_ambi = pd.read_csv('/content/data_ambi.csv')\n",
        "train_df_hard = pd.read_csv('/content/data_hard.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmlI_cCU3WVl",
        "outputId": "0564ec77-f23d-4ec2-8be7-cb9ec4cdab87"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 29.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 467 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s03ZWDhw26sA"
      },
      "source": [
        "train_easy_50_ambi_50_hard_100 = pd.concat([train_df_easy.iloc[:1750], train_df_ambi.iloc[:600],train_df_hard])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35NSrutw3fOJ"
      },
      "source": [
        "train_easy_50_ambi_50_hard_100.to_csv('train_easy_50_ambi_50_hard_100.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZyf77te3bYO",
        "outputId": "8a7cfe0f-2468-4d61-deec-35dc01bd4b4f"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/train_easy_50_ambi_50_hard_100.csv  --DEV_FILE /content/devnewdata_2.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/train_easy_50_ambi_50_hard_100.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 25.2kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 529kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 710kB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 1.15MB/s]\n",
            "Downloading: 100% 420M/420M [00:10<00:00, 40.2MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.1031, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.2500, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  59% 340/576 [05:29<03:48,  1.03it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  62% 360/576 [05:37<03:22,  1.07it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  66% 380/576 [05:44<02:57,  1.10it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  69% 400/576 [05:51<02:34,  1.14it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  73% 420/576 [05:58<02:13,  1.17it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  76% 440/576 [06:06<01:53,  1.20it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  80% 460/576 [06:13<01:34,  1.23it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  83% 480/576 [06:20<01:16,  1.26it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  87% 500/576 [06:27<00:58,  1.29it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  90% 520/576 [06:34<00:42,  1.32it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  94% 540/576 [06:42<00:26,  1.34it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Epoch 0:  97% 560/576 [06:49<00:11,  1.37it/s, loss=1.11, v_num=0, train_loss_step=1.100, train_acc_step=0.500]\n",
            "Validating:  98% 240/245 [01:26<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:28<00:00,  2.78it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0976, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3913, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 576/576 [06:58<00:00,  1.38it/s, loss=1.11, v_num=0, train_loss_step=1.060, train_acc_step=0.625]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1049, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3335, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1092804608 bytes == 0x56470e510000 @  0x7f2b7e4f1615 0x5646367244cc 0x56463680447a 0x56463672af0c 0x7f2b797439e4 0x7f2b7974bb14 0x7f2b79720a60 0x7f2ad0efef55 0x7f2ad0efa88e 0x7f2ad0f02235 0x7f2b79720fae 0x7f2b78e97aa8 0x564636728098 0x56463679b4d9 0x564636795ced 0x564636728bda 0x564636796915 0x5646367959ee 0x564636728bda 0x56463679ad00 0x564636728afa 0x564636796915 0x5646367959ee 0x564636728bda 0x564636796c0d 0x564636728afa 0x564636796c0d 0x5646367959ee 0x564636728bda 0x564636796c0d 0x5646367959ee\n",
            "tcmalloc: large alloc 1366007808 bytes == 0x564757df8000 @  0x7f2b7e4f1615 0x5646367244cc 0x56463680447a 0x56463672af0c 0x7f2b797439e4 0x7f2b7974bb14 0x7f2b79720a60 0x7f2ad0efef55 0x7f2ad0efa88e 0x7f2ad0f02235 0x7f2b79720fae 0x7f2b78e97aa8 0x564636728098 0x56463679b4d9 0x564636795ced 0x564636728bda 0x564636796915 0x5646367959ee 0x564636728bda 0x56463679ad00 0x564636728afa 0x564636796915 0x5646367959ee 0x564636728bda 0x564636796c0d 0x564636728afa 0x564636796c0d 0x5646367959ee 0x564636728bda 0x564636796c0d 0x5646367959ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x5646da352000 @  0x7f2b7e4f1615 0x5646367244cc 0x56463680447a 0x56463672af0c 0x7f2b797439e4 0x7f2b7974bb14 0x7f2b79720a60 0x7f2ad0efef55 0x7f2ad0efa88e 0x7f2ad0f02235 0x7f2b79720fae 0x7f2b78e97aa8 0x564636728098 0x56463679b4d9 0x564636795ced 0x564636728bda 0x564636796915 0x5646367959ee 0x564636728bda 0x56463679ad00 0x564636728afa 0x564636796915 0x5646367959ee 0x564636728bda 0x564636796c0d 0x564636728afa 0x564636796c0d 0x5646367959ee 0x564636728bda 0x564636796c0d 0x5646367959ee\n",
            "Epoch 1:  59% 340/576 [05:30<03:49,  1.03it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  62% 360/576 [05:38<03:23,  1.06it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  66% 380/576 [05:45<02:58,  1.10it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  69% 400/576 [05:52<02:35,  1.13it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  73% 420/576 [06:00<02:13,  1.17it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  76% 440/576 [06:07<01:53,  1.20it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  80% 460/576 [06:14<01:34,  1.23it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  83% 480/576 [06:21<01:16,  1.26it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  87% 500/576 [06:28<00:59,  1.29it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  90% 520/576 [06:35<00:42,  1.31it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  94% 540/576 [06:43<00:26,  1.34it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Epoch 1:  97% 560/576 [06:50<00:11,  1.36it/s, loss=1.09, v_num=0, train_loss_step=1.140, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "Validating:  98% 240/245 [01:26<00:01,  2.78it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:28<00:00,  2.77it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0708, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4306, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 576/576 [06:59<00:00,  1.37it/s, loss=1.09, v_num=0, train_loss_step=1.080, train_acc_step=0.250, train_loss_epoch=1.100, train_acc_epoch=0.333]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.0865, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3667, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 1: 100% 576/576 [07:00<00:00,  1.37it/s, loss=1.09, v_num=0, train_loss_step=1.080, train_acc_step=0.250, train_loss_epoch=1.100, train_acc_epoch=0.333]tcmalloc: large alloc 1707515904 bytes == 0x5646da352000 @  0x7f2b7e4f1615 0x5646367244cc 0x56463680447a 0x56463672af0c 0x7f2b797439e4 0x7f2b7974bb14 0x7f2b79720a60 0x7f2ad0efef55 0x7f2ad0efa88e 0x7f2ad0f02235 0x7f2b79720fae 0x7f2b78e97aa8 0x564636728098 0x56463679b4d9 0x564636795ced 0x564636728bda 0x564636796915 0x5646367959ee 0x564636728bda 0x56463679ad00 0x564636728afa 0x564636796915 0x5646367959ee 0x564636728bda 0x564636796c0d 0x564636728afa 0x564636796c0d 0x5646367959ee 0x564636728bda 0x564636796c0d 0x5646367959ee\n",
            "Epoch 2:  59% 340/576 [05:30<03:49,  1.03it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  62% 360/576 [05:38<03:22,  1.06it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  66% 380/576 [05:45<02:58,  1.10it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  69% 400/576 [05:52<02:35,  1.13it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  73% 420/576 [05:59<02:13,  1.17it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  76% 440/576 [06:06<01:53,  1.20it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  80% 460/576 [06:14<01:34,  1.23it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  83% 480/576 [06:21<01:16,  1.26it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  87% 500/576 [06:28<00:59,  1.29it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  90% 520/576 [06:35<00:42,  1.31it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  94% 540/576 [06:42<00:26,  1.34it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Epoch 2:  97% 560/576 [06:50<00:11,  1.37it/s, loss=1.07, v_num=0, train_loss_step=1.600, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "Validating:  98% 240/245 [01:26<00:01,  2.78it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:28<00:00,  2.78it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0617, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4362, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 576/576 [06:59<00:00,  1.37it/s, loss=1.11, v_num=0, train_loss_step=1.110, train_acc_step=0.250, train_loss_epoch=1.090, train_acc_epoch=0.367]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.0102, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.4872, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  59% 340/576 [05:30<03:49,  1.03it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  62% 360/576 [05:37<03:22,  1.07it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  66% 380/576 [05:45<02:58,  1.10it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  69% 400/576 [05:52<02:35,  1.14it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  73% 420/576 [05:59<02:13,  1.17it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  76% 440/576 [06:06<01:53,  1.20it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  80% 460/576 [06:14<01:34,  1.23it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  83% 480/576 [06:21<01:16,  1.26it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  87% 500/576 [06:28<00:59,  1.29it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  90% 520/576 [06:35<00:42,  1.31it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  94% 540/576 [06:42<00:26,  1.34it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Epoch 3:  97% 560/576 [06:50<00:11,  1.37it/s, loss=0.736, v_num=0, train_loss_step=0.629, train_acc_step=0.625, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.76it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.2959, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4531, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 576/576 [06:59<00:00,  1.37it/s, loss=0.812, v_num=0, train_loss_step=1.250, train_acc_step=0.500, train_loss_epoch=1.010, train_acc_epoch=0.487]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.7541, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.6828, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  59% 340/576 [05:30<03:49,  1.03it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  62% 360/576 [05:38<03:23,  1.06it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  66% 380/576 [05:45<02:58,  1.10it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  69% 400/576 [05:52<02:35,  1.13it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  73% 420/576 [06:00<02:13,  1.17it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  76% 440/576 [06:07<01:53,  1.20it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  80% 460/576 [06:14<01:34,  1.23it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  83% 480/576 [06:21<01:16,  1.26it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  87% 500/576 [06:28<00:59,  1.29it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  90% 520/576 [06:36<00:42,  1.31it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  94% 540/576 [06:43<00:26,  1.34it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Epoch 4:  97% 560/576 [06:50<00:11,  1.36it/s, loss=0.444, v_num=0, train_loss_step=0.495, train_acc_step=0.750, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.76it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.6295, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4515, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 576/576 [06:59<00:00,  1.37it/s, loss=0.393, v_num=0, train_loss_step=0.145, train_acc_step=1.000, train_loss_epoch=0.754, train_acc_epoch=0.683]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.5013, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.7991, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 576/576 [07:08<00:00,  1.35it/s, loss=0.393, v_num=0, train_loss_step=0.145, train_acc_step=1.000, train_loss_epoch=0.754, train_acc_epoch=0.683]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvldWfNozEll"
      },
      "source": [
        "train_ambi_75_hard_100 = pd.concat([train_df_ambi.iloc[:900], train_df_hard])\n",
        "train_ambi_75_hard_100.to_csv('train_ambi_75_hard_100.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yRQFASfzH1S",
        "outputId": "7c56b8da-ed48-445a-ee54-726b98102b6e"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/train_ambi_75_hard_100.csv  --DEV_FILE /content/devnewdata_2.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/train_ambi_75_hard_100.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.1031, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.2500, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  41% 160/395 [02:30<03:40,  1.07it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  46% 180/395 [02:37<03:08,  1.14it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  51% 200/395 [02:45<02:40,  1.21it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  56% 220/395 [02:52<02:17,  1.28it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  61% 240/395 [02:59<01:55,  1.34it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  66% 260/395 [03:06<01:36,  1.39it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  71% 280/395 [03:13<01:19,  1.44it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  76% 300/395 [03:21<01:03,  1.49it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  81% 320/395 [03:28<00:48,  1.54it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  86% 340/395 [03:35<00:34,  1.58it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  91% 360/395 [03:42<00:21,  1.62it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Epoch 0:  96% 380/395 [03:49<00:09,  1.65it/s, loss=1.11, v_num=1, train_loss_step=0.983, train_acc_step=0.500]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:28<00:00,  2.78it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0986, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3474, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 395/395 [03:58<00:00,  1.65it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.250]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1104, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3342, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1092804608 bytes == 0x5598e334a000 @  0x7f1ba9704615 0x5597c6e644cc 0x5597c6f4447a 0x5597c6e6af0c 0x7f1ba49569e4 0x7f1ba495eb14 0x7f1ba4933a60 0x7f1afc111f55 0x7f1afc10d88e 0x7f1afc115235 0x7f1ba4933fae 0x7f1ba40aaaa8 0x5597c6e68098 0x5597c6edb4d9 0x5597c6ed5ced 0x5597c6e68bda 0x5597c6ed6915 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6edad00 0x5597c6e68afa 0x5597c6ed6915 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6ed6c0d 0x5597c6e68afa 0x5597c6ed6c0d 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6ed6c0d 0x5597c6ed59ee\n",
            "tcmalloc: large alloc 1366007808 bytes == 0x55986a3ae000 @  0x7f1ba9704615 0x5597c6e644cc 0x5597c6f4447a 0x5597c6e6af0c 0x7f1ba49569e4 0x7f1ba495eb14 0x7f1ba4933a60 0x7f1afc111f55 0x7f1afc10d88e 0x7f1afc115235 0x7f1ba4933fae 0x7f1ba40aaaa8 0x5597c6e68098 0x5597c6edb4d9 0x5597c6ed5ced 0x5597c6e68bda 0x5597c6ed6915 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6edad00 0x5597c6e68afa 0x5597c6ed6915 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6ed6c0d 0x5597c6e68afa 0x5597c6ed6c0d 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6ed6c0d 0x5597c6ed59ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x5598e334a000 @  0x7f1ba9704615 0x5597c6e644cc 0x5597c6f4447a 0x5597c6e6af0c 0x7f1ba49569e4 0x7f1ba495eb14 0x7f1ba4933a60 0x7f1afc111f55 0x7f1afc10d88e 0x7f1afc115235 0x7f1ba4933fae 0x7f1ba40aaaa8 0x5597c6e68098 0x5597c6edb4d9 0x5597c6ed5ced 0x5597c6e68bda 0x5597c6ed6915 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6edad00 0x5597c6e68afa 0x5597c6ed6915 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6ed6c0d 0x5597c6e68afa 0x5597c6ed6c0d 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6ed6c0d 0x5597c6ed59ee\n",
            "Epoch 1:  41% 160/395 [02:31<03:41,  1.06it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  46% 180/395 [02:39<03:09,  1.13it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  51% 200/395 [02:46<02:42,  1.20it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  56% 220/395 [02:53<02:18,  1.27it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  61% 240/395 [03:00<01:56,  1.33it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  66% 260/395 [03:07<01:37,  1.38it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  71% 280/395 [03:15<01:20,  1.43it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  76% 300/395 [03:22<01:04,  1.48it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  81% 320/395 [03:29<00:49,  1.53it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  86% 340/395 [03:36<00:35,  1.57it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  91% 360/395 [03:44<00:21,  1.61it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 1:  96% 380/395 [03:51<00:09,  1.64it/s, loss=1.1, v_num=1, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.75it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0988, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3051, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 395/395 [04:00<00:00,  1.64it/s, loss=1.11, v_num=1, train_loss_step=1.040, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1167, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3283, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x55986a3ae000 @  0x7f1ba9704615 0x5597c6e644cc 0x5597c6f4447a 0x5597c6e6af0c 0x7f1ba49569e4 0x7f1ba495eb14 0x7f1ba4933a60 0x7f1afc111f55 0x7f1afc10d88e 0x7f1afc115235 0x7f1ba4933fae 0x7f1ba40aaaa8 0x5597c6e68098 0x5597c6edb4d9 0x5597c6ed5ced 0x5597c6e68bda 0x5597c6ed6915 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6edad00 0x5597c6e68afa 0x5597c6ed6915 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6ed6c0d 0x5597c6e68afa 0x5597c6ed6c0d 0x5597c6ed59ee 0x5597c6e68bda 0x5597c6ed6c0d 0x5597c6ed59ee\n",
            "Epoch 2:  41% 160/395 [02:30<03:41,  1.06it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  46% 180/395 [02:38<03:09,  1.13it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  51% 200/395 [02:45<02:41,  1.21it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  56% 220/395 [02:53<02:17,  1.27it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  61% 240/395 [03:00<01:56,  1.33it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  66% 260/395 [03:07<01:37,  1.39it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  71% 280/395 [03:14<01:20,  1.44it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  76% 300/395 [03:22<01:03,  1.48it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  81% 320/395 [03:29<00:49,  1.53it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  86% 340/395 [03:36<00:35,  1.57it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  91% 360/395 [03:43<00:21,  1.61it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Epoch 2:  96% 380/395 [03:50<00:09,  1.65it/s, loss=1.12, v_num=1, train_loss_step=1.070, train_acc_step=0.375, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.75it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0987, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3143, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 395/395 [04:00<00:00,  1.65it/s, loss=1.11, v_num=1, train_loss_step=1.120, train_acc_step=0.125, train_loss_epoch=1.120, train_acc_epoch=0.328]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1104, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3217, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  41% 160/395 [02:31<03:41,  1.06it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  46% 180/395 [02:38<03:09,  1.13it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  51% 200/395 [02:46<02:42,  1.20it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  56% 220/395 [02:53<02:17,  1.27it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  61% 240/395 [03:00<01:56,  1.33it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  66% 260/395 [03:07<01:37,  1.38it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  71% 280/395 [03:15<01:20,  1.43it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  76% 300/395 [03:22<01:04,  1.48it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  81% 320/395 [03:29<00:49,  1.53it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  86% 340/395 [03:36<00:35,  1.57it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  91% 360/395 [03:44<00:21,  1.61it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Epoch 3:  96% 380/395 [03:51<00:09,  1.64it/s, loss=1.1, v_num=1, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.75it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0988, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3214, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 395/395 [04:00<00:00,  1.64it/s, loss=1.1, v_num=1, train_loss_step=1.080, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.322]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1075, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3342, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  41% 160/395 [02:31<03:42,  1.06it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  46% 180/395 [02:39<03:10,  1.13it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  51% 200/395 [02:46<02:42,  1.20it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  56% 220/395 [02:53<02:18,  1.27it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  61% 240/395 [03:00<01:56,  1.33it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  66% 260/395 [03:08<01:37,  1.38it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  71% 280/395 [03:15<01:20,  1.43it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  76% 300/395 [03:22<01:04,  1.48it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  81% 320/395 [03:29<00:49,  1.52it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  86% 340/395 [03:37<00:35,  1.57it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  91% 360/395 [03:44<00:21,  1.61it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Epoch 4:  96% 380/395 [03:51<00:09,  1.64it/s, loss=1.12, v_num=1, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.75it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0988, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3189, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 395/395 [04:00<00:00,  1.64it/s, loss=1.11, v_num=1, train_loss_step=1.090, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1079, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3208, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 395/395 [04:09<00:00,  1.58it/s, loss=1.11, v_num=1, train_loss_step=1.090, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.334]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55vAxONK3nqu"
      },
      "source": [
        "train_easy_100_ambi_100 = pd.concat([train_df_easy, train_df_ambi])\n",
        "train_easy_100_ambi_100.to_csv('train_easy_100_ambi_100.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zHljmpzAiXY",
        "outputId": "cb09556d-73d7-4dc6-fbe7-549c884d546e"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/train_easy_100_ambi_100.csv --DEV_FILE /content/devnewdata_2.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/train_easy_100_ambi_100.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.1031, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.2500, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  72% 600/832 [12:11<04:42,  1.22s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  75% 620/832 [12:20<04:13,  1.19s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  77% 640/832 [12:29<03:44,  1.17s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  79% 660/832 [12:38<03:17,  1.15s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  82% 680/832 [12:47<02:51,  1.13s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  84% 700/832 [12:56<02:26,  1.11s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  87% 720/832 [13:05<02:02,  1.09s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  89% 740/832 [13:14<01:38,  1.07s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  91% 760/832 [13:23<01:16,  1.06s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  94% 780/832 [13:32<00:54,  1.04s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  96% 800/832 [13:41<00:32,  1.03s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Epoch 0:  99% 820/832 [13:50<00:12,  1.01s/it, loss=0.894, v_num=1, train_loss_step=0.942, train_acc_step=0.500]\n",
            "Validating:  98% 240/245 [01:48<00:02,  2.23it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:50<00:00,  2.24it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.1417, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4776, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 832/832 [14:01<00:00,  1.01s/it, loss=0.898, v_num=1, train_loss_step=0.697, train_acc_step=0.500]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.9248, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.5402, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1092804608 bytes == 0x56252282c000 @  0x7f6b0bf22615 0x5624066664cc 0x56240674647a 0x56240666cf0c 0x7f6b071749e4 0x7f6b0717cb14 0x7f6b07151a60 0x7f6a5e92ff55 0x7f6a5e92b88e 0x7f6a5e933235 0x7f6b07151fae 0x7f6b068c8aa8 0x56240666a098 0x5624066dd4d9 0x5624066d7ced 0x56240666abda 0x5624066d8915 0x5624066d79ee 0x56240666abda 0x5624066dcd00 0x56240666aafa 0x5624066d8915 0x5624066d79ee 0x56240666abda 0x5624066d8c0d 0x56240666aafa 0x5624066d8c0d 0x5624066d79ee 0x56240666abda 0x5624066d8c0d 0x5624066d79ee\n",
            "tcmalloc: large alloc 1366007808 bytes == 0x5624a9890000 @  0x7f6b0bf22615 0x5624066664cc 0x56240674647a 0x56240666cf0c 0x7f6b071749e4 0x7f6b0717cb14 0x7f6b07151a60 0x7f6a5e92ff55 0x7f6a5e92b88e 0x7f6a5e933235 0x7f6b07151fae 0x7f6b068c8aa8 0x56240666a098 0x5624066dd4d9 0x5624066d7ced 0x56240666abda 0x5624066d8915 0x5624066d79ee 0x56240666abda 0x5624066dcd00 0x56240666aafa 0x5624066d8915 0x5624066d79ee 0x56240666abda 0x5624066d8c0d 0x56240666aafa 0x5624066d8c0d 0x5624066d79ee 0x56240666abda 0x5624066d8c0d 0x5624066d79ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x56252282c000 @  0x7f6b0bf22615 0x5624066664cc 0x56240674647a 0x56240666cf0c 0x7f6b071749e4 0x7f6b0717cb14 0x7f6b07151a60 0x7f6a5e92ff55 0x7f6a5e92b88e 0x7f6a5e933235 0x7f6b07151fae 0x7f6b068c8aa8 0x56240666a098 0x5624066dd4d9 0x5624066d7ced 0x56240666abda 0x5624066d8915 0x5624066d79ee 0x56240666abda 0x5624066dcd00 0x56240666aafa 0x5624066d8915 0x5624066d79ee 0x56240666abda 0x5624066d8c0d 0x56240666aafa 0x5624066d8c0d 0x5624066d79ee 0x56240666abda 0x5624066d8c0d 0x5624066d79ee\n",
            "Epoch 1:  72% 600/832 [12:11<04:42,  1.22s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  75% 620/832 [12:20<04:13,  1.20s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  77% 640/832 [12:29<03:44,  1.17s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  79% 660/832 [12:38<03:17,  1.15s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  82% 680/832 [12:47<02:51,  1.13s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  84% 700/832 [12:56<02:26,  1.11s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  87% 720/832 [13:05<02:02,  1.09s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  89% 740/832 [13:14<01:38,  1.07s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  91% 760/832 [13:23<01:16,  1.06s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  94% 780/832 [13:32<00:54,  1.04s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  96% 800/832 [13:41<00:32,  1.03s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Epoch 1:  99% 820/832 [13:50<00:12,  1.01s/it, loss=0.695, v_num=1, train_loss_step=0.729, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "Validating:  98% 240/245 [01:48<00:02,  2.23it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:50<00:00,  2.22it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.3609, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4867, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 832/832 [14:02<00:00,  1.01s/it, loss=0.757, v_num=1, train_loss_step=0.598, train_acc_step=0.750, train_loss_epoch=0.925, train_acc_epoch=0.540]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.6567, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.7304, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x5624a9890000 @  0x7f6b0bf22615 0x5624066664cc 0x56240674647a 0x56240666cf0c 0x7f6b071749e4 0x7f6b0717cb14 0x7f6b07151a60 0x7f6a5e92ff55 0x7f6a5e92b88e 0x7f6a5e933235 0x7f6b07151fae 0x7f6b068c8aa8 0x56240666a098 0x5624066dd4d9 0x5624066d7ced 0x56240666abda 0x5624066d8915 0x5624066d79ee 0x56240666abda 0x5624066dcd00 0x56240666aafa 0x5624066d8915 0x5624066d79ee 0x56240666abda 0x5624066d8c0d 0x56240666aafa 0x5624066d8c0d 0x5624066d79ee 0x56240666abda 0x5624066d8c0d 0x5624066d79ee\n",
            "Epoch 2:  72% 600/832 [12:11<04:42,  1.22s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  75% 620/832 [12:21<04:13,  1.20s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  77% 640/832 [12:30<03:45,  1.17s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  79% 660/832 [12:39<03:17,  1.15s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  82% 680/832 [12:48<02:51,  1.13s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  84% 700/832 [12:56<02:26,  1.11s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  87% 720/832 [13:05<02:02,  1.09s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  89% 740/832 [13:14<01:38,  1.07s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  91% 760/832 [13:23<01:16,  1.06s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  94% 780/832 [13:32<00:54,  1.04s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  96% 800/832 [13:41<00:32,  1.03s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Epoch 2:  99% 820/832 [13:50<00:12,  1.01s/it, loss=0.343, v_num=1, train_loss_step=0.549, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "Validating:  98% 240/245 [01:48<00:02,  2.23it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:50<00:00,  2.23it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(2.1211, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4852, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 832/832 [14:02<00:00,  1.01s/it, loss=0.314, v_num=1, train_loss_step=0.527, train_acc_step=0.750, train_loss_epoch=0.657, train_acc_epoch=0.730]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.3424, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.8724, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  72% 600/832 [12:11<04:42,  1.22s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  75% 620/832 [12:20<04:13,  1.20s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  77% 640/832 [12:29<03:44,  1.17s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  79% 660/832 [12:38<03:17,  1.15s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  82% 680/832 [12:47<02:51,  1.13s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  84% 700/832 [12:56<02:26,  1.11s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  87% 720/832 [13:05<02:02,  1.09s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  89% 740/832 [13:14<01:38,  1.07s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  91% 760/832 [13:23<01:16,  1.06s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  94% 780/832 [13:32<00:54,  1.04s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  96% 800/832 [13:41<00:32,  1.03s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Epoch 3:  99% 820/832 [13:50<00:12,  1.01s/it, loss=0.1, v_num=1, train_loss_step=0.0959, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872]\n",
            "Validating:  98% 240/245 [01:48<00:02,  2.23it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:50<00:00,  2.22it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(3.3543, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4995, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 832/832 [14:01<00:00,  1.01s/it, loss=0.1, v_num=1, train_loss_step=0.013, train_acc_step=1.000, train_loss_epoch=0.342, train_acc_epoch=0.872] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.1275, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.9557, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  72% 600/832 [12:11<04:42,  1.22s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  75% 620/832 [12:21<04:13,  1.20s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  77% 640/832 [12:30<03:45,  1.17s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  79% 660/832 [12:39<03:17,  1.15s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  82% 680/832 [12:48<02:51,  1.13s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  84% 700/832 [12:57<02:26,  1.11s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  87% 720/832 [13:06<02:02,  1.09s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  89% 740/832 [13:15<01:38,  1.07s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  91% 760/832 [13:24<01:16,  1.06s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  94% 780/832 [13:33<00:54,  1.04s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  96% 800/832 [13:42<00:32,  1.03s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Epoch 4:  99% 820/832 [13:51<00:12,  1.01s/it, loss=0.0343, v_num=1, train_loss_step=0.0056, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "Validating:  98% 240/245 [01:48<00:02,  2.23it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:50<00:00,  2.22it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(3.5176, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4974, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 832/832 [14:02<00:00,  1.01s/it, loss=0.0332, v_num=1, train_loss_step=0.00043, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.0428, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.9859, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 832/832 [14:11<00:00,  1.02s/it, loss=0.0332, v_num=1, train_loss_step=0.00043, train_acc_step=1.000, train_loss_epoch=0.128, train_acc_epoch=0.956]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4tcKrdyAou7"
      },
      "source": [
        "train_ambi_50_hard_50 = pd.concat([train_df_ambi.iloc[:600], train_df_hard.iloc[:150]])\n",
        "train_ambi_50_hard_50.to_csv('train_ambi_50_hard_50.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQKLEee7FK4m",
        "outputId": "8debfc04-7c91-4864-a725-dcc1cb772faf"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/train_ambi_50_hard_50.csv --DEV_FILE /content/devnewdata_2.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/train_ambi_50_hard_50.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.1031, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.2500, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  30% 100/338 [01:32<03:40,  1.08it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  36% 120/338 [01:40<03:02,  1.20it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  41% 140/338 [01:47<02:32,  1.30it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  47% 160/338 [01:54<02:07,  1.39it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  53% 180/338 [02:01<01:47,  1.48it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  59% 200/338 [02:09<01:29,  1.55it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  65% 220/338 [02:16<01:13,  1.61it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  71% 240/338 [02:23<00:58,  1.67it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  77% 260/338 [02:30<00:45,  1.72it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  83% 280/338 [02:38<00:32,  1.77it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  89% 300/338 [02:45<00:20,  1.81it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Epoch 0:  95% 320/338 [02:52<00:09,  1.86it/s, loss=1.11, v_num=2, train_loss_step=1.090, train_acc_step=0.375]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:28<00:00,  2.78it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0988, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3418, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 338/338 [03:01<00:00,  1.86it/s, loss=1.1, v_num=2, train_loss_step=1.090, train_acc_step=0.375] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1063, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3280, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1092804608 bytes == 0x55748226e000 @  0x7fb927178615 0x557365f5b4cc 0x55736603b47a 0x557365f61f0c 0x7fb9223ca9e4 0x7fb9223d2b14 0x7fb9223a7a60 0x7fb879b85f55 0x7fb879b8188e 0x7fb879b89235 0x7fb9223a7fae 0x7fb921b1eaa8 0x557365f5f098 0x557365fd24d9 0x557365fccced 0x557365f5fbda 0x557365fcd915 0x557365fcc9ee 0x557365f5fbda 0x557365fd1d00 0x557365f5fafa 0x557365fcd915 0x557365fcc9ee 0x557365f5fbda 0x557365fcdc0d 0x557365f5fafa 0x557365fcdc0d 0x557365fcc9ee 0x557365f5fbda 0x557365fcdc0d 0x557365fcc9ee\n",
            "tcmalloc: large alloc 1366007808 bytes == 0x5574092d2000 @  0x7fb927178615 0x557365f5b4cc 0x55736603b47a 0x557365f61f0c 0x7fb9223ca9e4 0x7fb9223d2b14 0x7fb9223a7a60 0x7fb879b85f55 0x7fb879b8188e 0x7fb879b89235 0x7fb9223a7fae 0x7fb921b1eaa8 0x557365f5f098 0x557365fd24d9 0x557365fccced 0x557365f5fbda 0x557365fcd915 0x557365fcc9ee 0x557365f5fbda 0x557365fd1d00 0x557365f5fafa 0x557365fcd915 0x557365fcc9ee 0x557365f5fbda 0x557365fcdc0d 0x557365f5fafa 0x557365fcdc0d 0x557365fcc9ee 0x557365f5fbda 0x557365fcdc0d 0x557365fcc9ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x55748226e000 @  0x7fb927178615 0x557365f5b4cc 0x55736603b47a 0x557365f61f0c 0x7fb9223ca9e4 0x7fb9223d2b14 0x7fb9223a7a60 0x7fb879b85f55 0x7fb879b8188e 0x7fb879b89235 0x7fb9223a7fae 0x7fb921b1eaa8 0x557365f5f098 0x557365fd24d9 0x557365fccced 0x557365f5fbda 0x557365fcd915 0x557365fcc9ee 0x557365f5fbda 0x557365fd1d00 0x557365f5fafa 0x557365fcd915 0x557365fcc9ee 0x557365f5fbda 0x557365fcdc0d 0x557365f5fafa 0x557365fcdc0d 0x557365fcc9ee 0x557365f5fbda 0x557365fcdc0d 0x557365fcc9ee\n",
            "Epoch 1:  30% 100/338 [01:34<03:44,  1.06it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  36% 120/338 [01:42<03:05,  1.18it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  41% 140/338 [01:49<02:34,  1.28it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  47% 160/338 [01:56<02:09,  1.37it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  53% 180/338 [02:03<01:48,  1.45it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  59% 200/338 [02:10<01:30,  1.53it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  65% 220/338 [02:18<01:14,  1.59it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  71% 240/338 [02:25<00:59,  1.65it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  77% 260/338 [02:32<00:45,  1.70it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  83% 280/338 [02:39<00:33,  1.75it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  89% 300/338 [02:47<00:21,  1.80it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 1:  95% 320/338 [02:54<00:09,  1.84it/s, loss=1.13, v_num=2, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.75it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0986, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3439, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 338/338 [03:03<00:00,  1.84it/s, loss=1.11, v_num=2, train_loss_step=1.010, train_acc_step=0.750, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1046, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3468, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x5574092d2000 @  0x7fb927178615 0x557365f5b4cc 0x55736603b47a 0x557365f61f0c 0x7fb9223ca9e4 0x7fb9223d2b14 0x7fb9223a7a60 0x7fb879b85f55 0x7fb879b8188e 0x7fb879b89235 0x7fb9223a7fae 0x7fb921b1eaa8 0x557365f5f098 0x557365fd24d9 0x557365fccced 0x557365f5fbda 0x557365fcd915 0x557365fcc9ee 0x557365f5fbda 0x557365fd1d00 0x557365f5fafa 0x557365fcd915 0x557365fcc9ee 0x557365f5fbda 0x557365fcdc0d 0x557365f5fafa 0x557365fcdc0d 0x557365fcc9ee 0x557365f5fbda 0x557365fcdc0d 0x557365fcc9ee\n",
            "Epoch 2:  30% 100/338 [01:34<03:44,  1.06it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  36% 120/338 [01:42<03:05,  1.17it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  41% 140/338 [01:49<02:34,  1.28it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  47% 160/338 [01:56<02:09,  1.37it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  53% 180/338 [02:03<01:48,  1.45it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  59% 200/338 [02:10<01:30,  1.53it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  65% 220/338 [02:18<01:14,  1.59it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  71% 240/338 [02:25<00:59,  1.65it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  77% 260/338 [02:32<00:45,  1.70it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  83% 280/338 [02:39<00:33,  1.75it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  89% 300/338 [02:47<00:21,  1.80it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 2:  95% 320/338 [02:54<00:09,  1.84it/s, loss=1.08, v_num=2, train_loss_step=1.010, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.75it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0988, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3449, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 338/338 [03:03<00:00,  1.84it/s, loss=1.12, v_num=2, train_loss_step=1.050, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1100, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3495, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  30% 100/338 [01:33<03:43,  1.06it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  36% 120/338 [01:41<03:05,  1.18it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  41% 140/338 [01:49<02:34,  1.28it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  47% 160/338 [01:56<02:09,  1.37it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  53% 180/338 [02:03<01:48,  1.46it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  59% 200/338 [02:10<01:30,  1.53it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  65% 220/338 [02:18<01:14,  1.59it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  71% 240/338 [02:25<00:59,  1.65it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  77% 260/338 [02:32<00:45,  1.71it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  83% 280/338 [02:39<00:33,  1.75it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  89% 300/338 [02:46<00:21,  1.80it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Epoch 3:  95% 320/338 [02:54<00:09,  1.84it/s, loss=1.1, v_num=2, train_loss_step=1.140, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.76it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0992, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3459, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 338/338 [03:03<00:00,  1.84it/s, loss=1.11, v_num=2, train_loss_step=1.110, train_acc_step=0.375, train_loss_epoch=1.110, train_acc_epoch=0.349]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1033, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3481, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  30% 100/338 [01:34<03:44,  1.06it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  36% 120/338 [01:42<03:05,  1.17it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  41% 140/338 [01:49<02:34,  1.28it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  47% 160/338 [01:56<02:09,  1.37it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  53% 180/338 [02:03<01:48,  1.45it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  59% 200/338 [02:11<01:30,  1.53it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  65% 220/338 [02:18<01:14,  1.59it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  71% 240/338 [02:25<00:59,  1.65it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  77% 260/338 [02:32<00:45,  1.70it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  83% 280/338 [02:39<00:33,  1.75it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  89% 300/338 [02:47<00:21,  1.80it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Epoch 4:  95% 320/338 [02:54<00:09,  1.84it/s, loss=1.1, v_num=2, train_loss_step=1.080, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "Validating:  98% 240/245 [01:27<00:01,  2.77it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:29<00:00,  2.75it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.1002, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3209, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 338/338 [03:03<00:00,  1.84it/s, loss=1.11, v_num=2, train_loss_step=1.140, train_acc_step=0.125, train_loss_epoch=1.100, train_acc_epoch=0.348]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.0996, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3589, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 338/338 [03:12<00:00,  1.76it/s, loss=1.11, v_num=2, train_loss_step=1.140, train_acc_step=0.125, train_loss_epoch=1.100, train_acc_epoch=0.348]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AKlSxExFONn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}