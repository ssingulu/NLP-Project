{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgQ21un1JrOz",
        "outputId": "5abfd0aa-7c9b-48cf-cea6-a2100a8970bb"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/data_ambi.csv  --DEV_FILE /content/devnewdata_2.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/data_ambi.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 24.2kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 447kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 712kB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 1.15MB/s]\n",
            "Downloading: 100% 420M/420M [00:10<00:00, 40.9MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.1031, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.2500, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  41% 160/395 [02:33<03:44,  1.04it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  46% 180/395 [02:41<03:12,  1.12it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  51% 200/395 [02:48<02:44,  1.19it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  56% 220/395 [02:55<02:19,  1.25it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  61% 240/395 [03:03<01:58,  1.31it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  66% 260/395 [03:10<01:38,  1.36it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  71% 280/395 [03:18<01:21,  1.41it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  76% 300/395 [03:25<01:05,  1.46it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  81% 320/395 [03:32<00:49,  1.50it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  86% 340/395 [03:40<00:35,  1.54it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  91% 360/395 [03:47<00:22,  1.58it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Epoch 0:  96% 380/395 [03:55<00:09,  1.62it/s, loss=1.1, v_num=0, train_loss_step=1.110, train_acc_step=0.250]\n",
            "Validating:  98% 240/245 [01:29<00:01,  2.70it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:31<00:00,  2.71it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0995, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3168, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 395/395 [04:04<00:00,  1.62it/s, loss=1.12, v_num=0, train_loss_step=1.120, train_acc_step=0.250]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1126, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3258, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1092804608 bytes == 0x55e4e1c44000 @  0x7f3a3a7fd615 0x55e40ab9c4cc 0x55e40ac7c47a 0x55e40aba2f0c 0x7f3a35a4f9e4 0x7f3a35a57b14 0x7f3a35a2ca60 0x7f398d20af55 0x7f398d20688e 0x7f398d20e235 0x7f3a35a2cfae 0x7f3a351a3aa8 0x55e40aba0098 0x55e40ac134d9 0x55e40ac0dced 0x55e40aba0bda 0x55e40ac0e915 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac12d00 0x55e40aba0afa 0x55e40ac0e915 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac0ec0d 0x55e40aba0afa 0x55e40ac0ec0d 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac0ec0d 0x55e40ac0d9ee\n",
            "Epoch 0: 100% 395/395 [04:07<00:00,  1.60it/s, loss=1.12, v_num=0, train_loss_step=1.120, train_acc_step=0.250]tcmalloc: large alloc 1366007808 bytes == 0x55e52b52c000 @  0x7f3a3a7fd615 0x55e40ab9c4cc 0x55e40ac7c47a 0x55e40aba2f0c 0x7f3a35a4f9e4 0x7f3a35a57b14 0x7f3a35a2ca60 0x7f398d20af55 0x7f398d20688e 0x7f398d20e235 0x7f3a35a2cfae 0x7f3a351a3aa8 0x55e40aba0098 0x55e40ac134d9 0x55e40ac0dced 0x55e40aba0bda 0x55e40ac0e915 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac12d00 0x55e40aba0afa 0x55e40ac0e915 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac0ec0d 0x55e40aba0afa 0x55e40ac0ec0d 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac0ec0d 0x55e40ac0d9ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x55e4ada86000 @  0x7f3a3a7fd615 0x55e40ab9c4cc 0x55e40ac7c47a 0x55e40aba2f0c 0x7f3a35a4f9e4 0x7f3a35a57b14 0x7f3a35a2ca60 0x7f398d20af55 0x7f398d20688e 0x7f398d20e235 0x7f3a35a2cfae 0x7f3a351a3aa8 0x55e40aba0098 0x55e40ac134d9 0x55e40ac0dced 0x55e40aba0bda 0x55e40ac0e915 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac12d00 0x55e40aba0afa 0x55e40ac0e915 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac0ec0d 0x55e40aba0afa 0x55e40ac0ec0d 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac0ec0d 0x55e40ac0d9ee\n",
            "Epoch 1:  41% 160/395 [02:33<03:45,  1.04it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  46% 180/395 [02:41<03:13,  1.11it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  51% 200/395 [02:49<02:44,  1.18it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  56% 220/395 [02:56<02:20,  1.25it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  61% 240/395 [03:03<01:58,  1.31it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  66% 260/395 [03:11<01:39,  1.36it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  71% 280/395 [03:18<01:21,  1.41it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  76% 300/395 [03:26<01:05,  1.46it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  81% 320/395 [03:33<00:50,  1.50it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  86% 340/395 [03:40<00:35,  1.54it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  91% 360/395 [03:48<00:22,  1.58it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Epoch 1:  96% 380/395 [03:55<00:09,  1.61it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.625, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "Validating:  98% 240/245 [01:29<00:01,  2.70it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:31<00:00,  2.69it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0986, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3327, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 395/395 [04:05<00:00,  1.61it/s, loss=1.09, v_num=0, train_loss_step=1.060, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.326]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1111, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3283, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x55e4ada86000 @  0x7f3a3a7fd615 0x55e40ab9c4cc 0x55e40ac7c47a 0x55e40aba2f0c 0x7f3a35a4f9e4 0x7f3a35a57b14 0x7f3a35a2ca60 0x7f398d20af55 0x7f398d20688e 0x7f398d20e235 0x7f3a35a2cfae 0x7f3a351a3aa8 0x55e40aba0098 0x55e40ac134d9 0x55e40ac0dced 0x55e40aba0bda 0x55e40ac0e915 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac12d00 0x55e40aba0afa 0x55e40ac0e915 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac0ec0d 0x55e40aba0afa 0x55e40ac0ec0d 0x55e40ac0d9ee 0x55e40aba0bda 0x55e40ac0ec0d 0x55e40ac0d9ee\n",
            "Epoch 2:  41% 160/395 [02:33<03:45,  1.04it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  46% 180/395 [02:41<03:12,  1.12it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  51% 200/395 [02:48<02:44,  1.19it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  56% 220/395 [02:56<02:20,  1.25it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  61% 240/395 [03:03<01:58,  1.31it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  66% 260/395 [03:10<01:39,  1.36it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  71% 280/395 [03:18<01:21,  1.41it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  76% 300/395 [03:25<01:05,  1.46it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  81% 320/395 [03:33<00:49,  1.50it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  86% 340/395 [03:40<00:35,  1.54it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  91% 360/395 [03:47<00:22,  1.58it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Epoch 2:  96% 380/395 [03:55<00:09,  1.61it/s, loss=1.1, v_num=0, train_loss_step=1.090, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "Validating:  98% 240/245 [01:29<00:01,  2.70it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:31<00:00,  2.70it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0986, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3551, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 395/395 [04:04<00:00,  1.61it/s, loss=1.1, v_num=0, train_loss_step=1.160, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.328]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1102, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3392, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  41% 160/395 [02:33<03:45,  1.04it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  46% 180/395 [02:41<03:13,  1.11it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  51% 200/395 [02:49<02:44,  1.18it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  56% 220/395 [02:56<02:20,  1.25it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  61% 240/395 [03:03<01:58,  1.30it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  66% 260/395 [03:11<01:39,  1.36it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  71% 280/395 [03:18<01:21,  1.41it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  76% 300/395 [03:26<01:05,  1.46it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  81% 320/395 [03:33<00:50,  1.50it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  86% 340/395 [03:40<00:35,  1.54it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  91% 360/395 [03:48<00:22,  1.58it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Epoch 3:  96% 380/395 [03:55<00:09,  1.61it/s, loss=1.1, v_num=0, train_loss_step=1.100, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "Validating:  98% 240/245 [01:29<00:01,  2.70it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:31<00:00,  2.69it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0986, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3480, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 395/395 [04:05<00:00,  1.61it/s, loss=1.11, v_num=0, train_loss_step=1.110, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.339]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1025, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3475, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  41% 160/395 [02:34<03:46,  1.04it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  46% 180/395 [02:42<03:13,  1.11it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  51% 200/395 [02:49<02:45,  1.18it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  56% 220/395 [02:56<02:20,  1.24it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  61% 240/395 [03:04<01:59,  1.30it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  66% 260/395 [03:11<01:39,  1.36it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  71% 280/395 [03:19<01:21,  1.41it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  76% 300/395 [03:26<01:05,  1.45it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  81% 320/395 [03:33<00:50,  1.50it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  86% 340/395 [03:41<00:35,  1.54it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  91% 360/395 [03:48<00:22,  1.57it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Epoch 4:  96% 380/395 [03:56<00:09,  1.61it/s, loss=1.09, v_num=0, train_loss_step=1.100, train_acc_step=0.375, train_loss_epoch=1.100, train_acc_epoch=0.347]\n",
            "Validating:  98% 240/245 [01:29<00:01,  2.70it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:31<00:00,  2.69it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0986, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3388, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 395/395 [04:05<00:00,  1.61it/s, loss=1.1, v_num=0, train_loss_step=1.130, train_acc_step=0.250, train_loss_epoch=1.100, train_acc_epoch=0.347] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1046, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3375, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 395/395 [04:13<00:00,  1.56it/s, loss=1.1, v_num=0, train_loss_step=1.130, train_acc_step=0.250, train_loss_epoch=1.100, train_acc_epoch=0.347]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfmlkgo3KAJB",
        "outputId": "e1008444-6bf2-4220-88fa-a0aa62869f9d"
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.5.4-py3-none-any.whl (524 kB)\n",
            "\u001b[K     |████████████████████████████████| 524 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 30.6 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 27.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.42.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 45.9 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 50.4 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 40.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.8)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=1f6d1482ab95731c51bdcacfd5652507699630cc4b0a712f49951997c652ab53\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.4 torchmetrics-0.6.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAFS9pFuKK01",
        "outputId": "ef0c3b53-035a-4d60-c4c1-7dabed07cc46"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 468 kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVWC1U8fQ277"
      },
      "source": [
        "import pandas as pd\n",
        "train_df_easy = pd.read_csv('/content/data_easy.csv')\n",
        "train_df_ambi = pd.read_csv('/content/data_ambi.csv')\n",
        "train_df_hard = pd.read_csv('/content/data_hard.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NqLCwImfger"
      },
      "source": [
        "train_easy_75_hard_100 = pd.concat([train_df_easy.iloc[:2625], train_df_hard])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz9n_EuBfht0"
      },
      "source": [
        "train_easy_75_hard_100.to_csv('train_easy_75_hard_100.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "cWWmWFI3Q5eE",
        "outputId": "dc89db58-8251-4514-be92-2d2dae27da39"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.scatterplot(x= difficulty, y = confidence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1ec802bc90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR2Bg1J9RA3_",
        "outputId": "06f9d90c-b532-4b7e-ba12-d0a8cabfd69e"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/latestdata.csv  --DEV_FILE /content/latestdata.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/latestdata.csv', TRAIN_FILE='/content/latestdata.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.0944, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.5000, device='cuda:0')\n",
            "Writing predictions for /content/latestdata.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  51% 640/1250 [10:38<10:08,  1.00it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/625 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  53% 660/1250 [10:45<09:37,  1.02it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  54% 680/1250 [10:53<09:07,  1.04it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  56% 700/1250 [11:00<08:39,  1.06it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  58% 720/1250 [11:08<08:11,  1.08it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  59% 740/1250 [11:15<07:45,  1.10it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  61% 760/1250 [11:22<07:20,  1.11it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  62% 780/1250 [11:30<06:56,  1.13it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  64% 800/1250 [11:37<06:32,  1.15it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  66% 820/1250 [11:45<06:09,  1.16it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  67% 840/1250 [11:52<05:47,  1.18it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  69% 860/1250 [12:00<05:26,  1.19it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  70% 880/1250 [12:07<05:05,  1.21it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  72% 900/1250 [12:14<04:45,  1.22it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  74% 920/1250 [12:22<04:26,  1.24it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  75% 940/1250 [12:29<04:07,  1.25it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  77% 960/1250 [12:37<03:48,  1.27it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  78% 980/1250 [12:44<03:30,  1.28it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  80% 1000/1250 [12:52<03:13,  1.30it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  82% 1020/1250 [12:59<02:55,  1.31it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  83% 1040/1250 [13:06<02:38,  1.32it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  85% 1060/1250 [13:14<02:22,  1.33it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  86% 1080/1250 [13:21<02:06,  1.35it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  88% 1100/1250 [13:29<01:50,  1.36it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  90% 1120/1250 [13:36<01:34,  1.37it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  91% 1140/1250 [13:43<01:19,  1.38it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  93% 1160/1250 [13:51<01:04,  1.40it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  94% 1180/1250 [13:58<00:49,  1.41it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  96% 1200/1250 [14:06<00:35,  1.42it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  98% 1220/1250 [14:13<00:20,  1.43it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Epoch 0:  99% 1240/1250 [14:21<00:06,  1.44it/s, loss=1.09, v_num=1, train_loss_step=1.170, train_acc_step=0.375]\n",
            "Validating:  99% 620/625 [03:50<00:01,  2.69it/s]\u001b[A\n",
            "Validating: 100% 625/625 [03:52<00:00,  2.67it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(0.8932, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.6716, device='cuda:0')\n",
            "Writing predictions for /content/latestdata.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 1250/1250 [14:30<00:00,  1.44it/s, loss=1.09, v_num=1, train_loss_step=0.897, train_acc_step=0.500]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.9908, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.4926, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1092804608 bytes == 0x5615afbb8000 @  0x7f3a5fbe5615 0x5614933a84cc 0x56149348847a 0x5614933aef0c 0x7f3a5ae379e4 0x7f3a5ae3fb14 0x7f3a5ae14a60 0x7f39b25f2f55 0x7f39b25ee88e 0x7f39b25f6235 0x7f3a5ae14fae 0x7f3a5a58baa8 0x5614933ac098 0x56149341f4d9 0x561493419ced 0x5614933acbda 0x56149341a915 0x5614934199ee 0x5614933acbda 0x56149341ed00 0x5614933acafa 0x56149341a915 0x5614934199ee 0x5614933acbda 0x56149341ac0d 0x5614933acafa 0x56149341ac0d 0x5614934199ee 0x5614933acbda 0x56149341ac0d 0x5614934199ee\n",
            "tcmalloc: large alloc 1366007808 bytes == 0x561536c1c000 @  0x7f3a5fbe5615 0x5614933a84cc 0x56149348847a 0x5614933aef0c 0x7f3a5ae379e4 0x7f3a5ae3fb14 0x7f3a5ae14a60 0x7f39b25f2f55 0x7f39b25ee88e 0x7f39b25f6235 0x7f3a5ae14fae 0x7f3a5a58baa8 0x5614933ac098 0x56149341f4d9 0x561493419ced 0x5614933acbda 0x56149341a915 0x5614934199ee 0x5614933acbda 0x56149341ed00 0x5614933acafa 0x56149341a915 0x5614934199ee 0x5614933acbda 0x56149341ac0d 0x5614933acafa 0x56149341ac0d 0x5614934199ee 0x5614933acbda 0x56149341ac0d 0x5614934199ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x5615afbb8000 @  0x7f3a5fbe5615 0x5614933a84cc 0x56149348847a 0x5614933aef0c 0x7f3a5ae379e4 0x7f3a5ae3fb14 0x7f3a5ae14a60 0x7f39b25f2f55 0x7f39b25ee88e 0x7f39b25f6235 0x7f3a5ae14fae 0x7f3a5a58baa8 0x5614933ac098 0x56149341f4d9 0x561493419ced 0x5614933acbda 0x56149341a915 0x5614934199ee 0x5614933acbda 0x56149341ed00 0x5614933acafa 0x56149341a915 0x5614934199ee 0x5614933acbda 0x56149341ac0d 0x5614933acafa 0x56149341ac0d 0x5614934199ee 0x5614933acbda 0x56149341ac0d 0x5614934199ee\n",
            "Epoch 1:  51% 640/1250 [10:36<10:06,  1.01it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/625 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  53% 660/1250 [10:44<09:36,  1.02it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  54% 680/1250 [10:52<09:06,  1.04it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  56% 700/1250 [10:59<08:38,  1.06it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  58% 720/1250 [11:07<08:11,  1.08it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  59% 740/1250 [11:14<07:44,  1.10it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  61% 760/1250 [11:21<07:19,  1.11it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  62% 780/1250 [11:29<06:55,  1.13it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  64% 800/1250 [11:36<06:31,  1.15it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  66% 820/1250 [11:44<06:09,  1.16it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  67% 840/1250 [11:51<05:47,  1.18it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  69% 860/1250 [11:59<05:26,  1.20it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  70% 880/1250 [12:06<05:05,  1.21it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  72% 900/1250 [12:13<04:45,  1.23it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  74% 920/1250 [12:21<04:25,  1.24it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  75% 940/1250 [12:28<04:06,  1.26it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  77% 960/1250 [12:36<03:48,  1.27it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  78% 980/1250 [12:43<03:30,  1.28it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  80% 1000/1250 [12:50<03:12,  1.30it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  82% 1020/1250 [12:58<02:55,  1.31it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  83% 1040/1250 [13:05<02:38,  1.32it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  85% 1060/1250 [13:13<02:22,  1.34it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  86% 1080/1250 [13:20<02:06,  1.35it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  88% 1100/1250 [13:28<01:50,  1.36it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  90% 1120/1250 [13:35<01:34,  1.37it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  91% 1140/1250 [13:42<01:19,  1.39it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  93% 1160/1250 [13:50<01:04,  1.40it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  94% 1180/1250 [13:57<00:49,  1.41it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  96% 1200/1250 [14:05<00:35,  1.42it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  98% 1220/1250 [14:12<00:20,  1.43it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Epoch 1:  99% 1240/1250 [14:20<00:06,  1.44it/s, loss=0.675, v_num=1, train_loss_step=0.518, train_acc_step=0.875, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "Validating:  99% 620/625 [03:50<00:01,  2.70it/s]\u001b[A\n",
            "Validating: 100% 625/625 [03:53<00:00,  2.65it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(0.4329, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.8510, device='cuda:0')\n",
            "Writing predictions for /content/latestdata.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 1250/1250 [14:29<00:00,  1.44it/s, loss=0.714, v_num=1, train_loss_step=0.953, train_acc_step=0.375, train_loss_epoch=0.991, train_acc_epoch=0.493]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.7725, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.6550, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x561536c1c000 @  0x7f3a5fbe5615 0x5614933a84cc 0x56149348847a 0x5614933aef0c 0x7f3a5ae379e4 0x7f3a5ae3fb14 0x7f3a5ae14a60 0x7f39b25f2f55 0x7f39b25ee88e 0x7f39b25f6235 0x7f3a5ae14fae 0x7f3a5a58baa8 0x5614933ac098 0x56149341f4d9 0x561493419ced 0x5614933acbda 0x56149341a915 0x5614934199ee 0x5614933acbda 0x56149341ed00 0x5614933acafa 0x56149341a915 0x5614934199ee 0x5614933acbda 0x56149341ac0d 0x5614933acafa 0x56149341ac0d 0x5614934199ee 0x5614933acbda 0x56149341ac0d 0x5614934199ee\n",
            "Epoch 2:  51% 640/1250 [10:35<10:06,  1.01it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/625 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  53% 660/1250 [10:43<09:35,  1.02it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  54% 680/1250 [10:51<09:05,  1.04it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  56% 700/1250 [10:58<08:37,  1.06it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  58% 720/1250 [11:06<08:10,  1.08it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  59% 740/1250 [11:13<07:44,  1.10it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  61% 760/1250 [11:21<07:19,  1.12it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  62% 780/1250 [11:28<06:54,  1.13it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  64% 800/1250 [11:35<06:31,  1.15it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  66% 820/1250 [11:43<06:08,  1.17it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  67% 840/1250 [11:50<05:46,  1.18it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  69% 860/1250 [11:58<05:25,  1.20it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  70% 880/1250 [12:05<05:05,  1.21it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  72% 900/1250 [12:13<04:45,  1.23it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  74% 920/1250 [12:20<04:25,  1.24it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  75% 940/1250 [12:27<04:06,  1.26it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  77% 960/1250 [12:35<03:48,  1.27it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  78% 980/1250 [12:42<03:30,  1.28it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  80% 1000/1250 [12:50<03:12,  1.30it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  82% 1020/1250 [12:57<02:55,  1.31it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  83% 1040/1250 [13:04<02:38,  1.32it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  85% 1060/1250 [13:12<02:22,  1.34it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  86% 1080/1250 [13:19<02:05,  1.35it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  88% 1100/1250 [13:27<01:50,  1.36it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  90% 1120/1250 [13:34<01:34,  1.37it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  91% 1140/1250 [13:42<01:19,  1.39it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  93% 1160/1250 [13:49<01:04,  1.40it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  94% 1180/1250 [13:56<00:49,  1.41it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  96% 1200/1250 [14:04<00:35,  1.42it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  98% 1220/1250 [14:11<00:20,  1.43it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Epoch 2:  99% 1240/1250 [14:19<00:06,  1.44it/s, loss=0.514, v_num=1, train_loss_step=0.655, train_acc_step=0.625, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "Validating:  99% 620/625 [03:50<00:01,  2.69it/s]\u001b[A\n",
            "Validating: 100% 625/625 [03:53<00:00,  2.65it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(0.1336, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.9610, device='cuda:0')\n",
            "Writing predictions for /content/latestdata.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 1250/1250 [14:28<00:00,  1.44it/s, loss=0.481, v_num=1, train_loss_step=0.648, train_acc_step=0.750, train_loss_epoch=0.772, train_acc_epoch=0.655]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.4555, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.8218, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  51% 640/1250 [10:35<10:05,  1.01it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/625 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  53% 660/1250 [10:43<09:35,  1.03it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  54% 680/1250 [10:51<09:05,  1.04it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  56% 700/1250 [10:58<08:37,  1.06it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  58% 720/1250 [11:06<08:10,  1.08it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  59% 740/1250 [11:13<07:44,  1.10it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  61% 760/1250 [11:21<07:19,  1.12it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  62% 780/1250 [11:28<06:54,  1.13it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  64% 800/1250 [11:35<06:31,  1.15it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  66% 820/1250 [11:43<06:08,  1.17it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  67% 840/1250 [11:50<05:46,  1.18it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  69% 860/1250 [11:58<05:25,  1.20it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  70% 880/1250 [12:05<05:05,  1.21it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  72% 900/1250 [12:13<04:45,  1.23it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  74% 920/1250 [12:20<04:25,  1.24it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  75% 940/1250 [12:27<04:06,  1.26it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  77% 960/1250 [12:35<03:48,  1.27it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  78% 980/1250 [12:42<03:30,  1.28it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  80% 1000/1250 [12:50<03:12,  1.30it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  82% 1020/1250 [12:57<02:55,  1.31it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  83% 1040/1250 [13:05<02:38,  1.32it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  85% 1060/1250 [13:12<02:22,  1.34it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  86% 1080/1250 [13:19<02:05,  1.35it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  88% 1100/1250 [13:27<01:50,  1.36it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  90% 1120/1250 [13:34<01:34,  1.37it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  91% 1140/1250 [13:42<01:19,  1.39it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  93% 1160/1250 [13:49<01:04,  1.40it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  94% 1180/1250 [13:57<00:49,  1.41it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  96% 1200/1250 [14:04<00:35,  1.42it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  98% 1220/1250 [14:11<00:20,  1.43it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Epoch 3:  99% 1240/1250 [14:19<00:06,  1.44it/s, loss=0.225, v_num=1, train_loss_step=0.121, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "Validating:  99% 620/625 [03:50<00:01,  2.70it/s]\u001b[A\n",
            "Validating: 100% 625/625 [03:53<00:00,  2.65it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(0.0446, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.9880, device='cuda:0')\n",
            "Writing predictions for /content/latestdata.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 1250/1250 [14:29<00:00,  1.44it/s, loss=0.225, v_num=1, train_loss_step=0.130, train_acc_step=1.000, train_loss_epoch=0.455, train_acc_epoch=0.822]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.2193, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.9218, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  51% 640/1250 [10:36<10:06,  1.01it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/625 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  53% 660/1250 [10:44<09:35,  1.02it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  54% 680/1250 [10:51<09:06,  1.04it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  56% 700/1250 [10:59<08:37,  1.06it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  58% 720/1250 [11:06<08:10,  1.08it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  59% 740/1250 [11:13<07:44,  1.10it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  61% 760/1250 [11:21<07:19,  1.12it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  62% 780/1250 [11:28<06:55,  1.13it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  64% 800/1250 [11:36<06:31,  1.15it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  66% 820/1250 [11:43<06:08,  1.17it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  67% 840/1250 [11:51<05:47,  1.18it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  69% 860/1250 [11:58<05:25,  1.20it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  70% 880/1250 [12:05<05:05,  1.21it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  72% 900/1250 [12:13<04:45,  1.23it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  74% 920/1250 [12:20<04:25,  1.24it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  75% 940/1250 [12:28<04:06,  1.26it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  77% 960/1250 [12:35<03:48,  1.27it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  78% 980/1250 [12:43<03:30,  1.28it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  80% 1000/1250 [12:50<03:12,  1.30it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  82% 1020/1250 [12:57<02:55,  1.31it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  83% 1040/1250 [13:05<02:38,  1.32it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  85% 1060/1250 [13:12<02:22,  1.34it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  86% 1080/1250 [13:20<02:05,  1.35it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  88% 1100/1250 [13:27<01:50,  1.36it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  90% 1120/1250 [13:34<01:34,  1.37it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  91% 1140/1250 [13:42<01:19,  1.39it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  93% 1160/1250 [13:49<01:04,  1.40it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  94% 1180/1250 [13:57<00:49,  1.41it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  96% 1200/1250 [14:04<00:35,  1.42it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  98% 1220/1250 [14:12<00:20,  1.43it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Epoch 4:  99% 1240/1250 [14:19<00:06,  1.44it/s, loss=0.0572, v_num=1, train_loss_step=0.0066, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n",
            "Validating:  99% 620/625 [03:50<00:01,  2.69it/s]\u001b[A\n",
            "Validating: 100% 625/625 [03:53<00:00,  2.65it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(0.0215, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.9932, device='cuda:0')\n",
            "Writing predictions for /content/latestdata.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 1250/1250 [14:29<00:00,  1.44it/s, loss=0.109, v_num=1, train_loss_step=0.0766, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.0889, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.9688, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 1250/1250 [14:38<00:00,  1.42it/s, loss=0.109, v_num=1, train_loss_step=0.0766, train_acc_step=1.000, train_loss_epoch=0.219, train_acc_epoch=0.922]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYNqoriKftwE",
        "outputId": "6070ca96-9bf9-4ac7-9731-c6a977b2a520"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/train_easy_75_hard_100.csv  --DEV_FILE /content/devnewdata_2.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/train_easy_75_hard_100.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 14.8kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 584kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 2.06MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 3.13MB/s]\n",
            "Downloading: 100% 420M/420M [00:27<00:00, 16.3MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.1031, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.2500, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  62% 380/610 [07:29<04:32,  1.18s/it, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  66% 400/610 [07:38<04:00,  1.15s/it, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  69% 420/610 [07:47<03:31,  1.11s/it, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  72% 440/610 [07:56<03:04,  1.08s/it, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  75% 460/610 [08:05<02:38,  1.06s/it, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  79% 480/610 [08:14<02:13,  1.03s/it, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  82% 500/610 [08:22<01:50,  1.01s/it, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  85% 520/610 [08:31<01:28,  1.02it/s, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  89% 540/610 [08:40<01:07,  1.04it/s, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  92% 560/610 [08:49<00:47,  1.06it/s, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  95% 580/610 [08:58<00:27,  1.08it/s, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Epoch 0:  98% 600/610 [09:06<00:09,  1.10it/s, loss=1.1, v_num=0, train_loss_step=1.170, train_acc_step=0.250]\n",
            "Validating:  98% 240/245 [01:46<00:02,  2.27it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.28it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0985, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3694, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 610/610 [09:17<00:00,  1.09it/s, loss=1.1, v_num=0, train_loss_step=1.120, train_acc_step=0.375]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.1116, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3360, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1092804608 bytes == 0x55a7502a0000 @  0x7fc22b199615 0x55a6798424cc 0x55a67992247a 0x55a679848f0c 0x7fc2263eb9e4 0x7fc2263f3b14 0x7fc2263c8a60 0x7fc17dba6f55 0x7fc17dba288e 0x7fc17dbaa235 0x7fc2263c8fae 0x7fc225b3faa8 0x55a679846098 0x55a6798b94d9 0x55a6798b3ced 0x55a679846bda 0x55a6798b4915 0x55a6798b39ee 0x55a679846bda 0x55a6798b8d00 0x55a679846afa 0x55a6798b4915 0x55a6798b39ee 0x55a679846bda 0x55a6798b4c0d 0x55a679846afa 0x55a6798b4c0d 0x55a6798b39ee 0x55a679846bda 0x55a6798b4c0d 0x55a6798b39ee\n",
            "tcmalloc: large alloc 1366007808 bytes == 0x55a799b88000 @  0x7fc22b199615 0x55a6798424cc 0x55a67992247a 0x55a679848f0c 0x7fc2263eb9e4 0x7fc2263f3b14 0x7fc2263c8a60 0x7fc17dba6f55 0x7fc17dba288e 0x7fc17dbaa235 0x7fc2263c8fae 0x7fc225b3faa8 0x55a679846098 0x55a6798b94d9 0x55a6798b3ced 0x55a679846bda 0x55a6798b4915 0x55a6798b39ee 0x55a679846bda 0x55a6798b8d00 0x55a679846afa 0x55a6798b4915 0x55a6798b39ee 0x55a679846bda 0x55a6798b4c0d 0x55a679846afa 0x55a6798b4c0d 0x55a6798b39ee 0x55a679846bda 0x55a6798b4c0d 0x55a6798b39ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x55a71c0e2000 @  0x7fc22b199615 0x55a6798424cc 0x55a67992247a 0x55a679848f0c 0x7fc2263eb9e4 0x7fc2263f3b14 0x7fc2263c8a60 0x7fc17dba6f55 0x7fc17dba288e 0x7fc17dbaa235 0x7fc2263c8fae 0x7fc225b3faa8 0x55a679846098 0x55a6798b94d9 0x55a6798b3ced 0x55a679846bda 0x55a6798b4915 0x55a6798b39ee 0x55a679846bda 0x55a6798b8d00 0x55a679846afa 0x55a6798b4915 0x55a6798b39ee 0x55a679846bda 0x55a6798b4c0d 0x55a679846afa 0x55a6798b4c0d 0x55a6798b39ee 0x55a679846bda 0x55a6798b4c0d 0x55a6798b39ee\n",
            "Epoch 1:  62% 380/610 [07:29<04:31,  1.18s/it, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  66% 400/610 [07:38<04:00,  1.15s/it, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  69% 420/610 [07:47<03:31,  1.11s/it, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  72% 440/610 [07:56<03:03,  1.08s/it, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  75% 460/610 [08:04<02:38,  1.05s/it, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  79% 480/610 [08:13<02:13,  1.03s/it, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  82% 500/610 [08:22<01:50,  1.01s/it, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  85% 520/610 [08:31<01:28,  1.02it/s, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  89% 540/610 [08:40<01:07,  1.04it/s, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  92% 560/610 [08:48<00:47,  1.06it/s, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  95% 580/610 [08:57<00:27,  1.08it/s, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Epoch 1:  98% 600/610 [09:06<00:09,  1.10it/s, loss=1.12, v_num=0, train_loss_step=1.020, train_acc_step=0.500, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "Validating:  98% 240/245 [01:46<00:02,  2.27it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.26it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0983, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.3959, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 610/610 [09:17<00:00,  1.09it/s, loss=1.12, v_num=0, train_loss_step=1.110, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.336]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.0969, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.3589, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 1: 100% 610/610 [09:18<00:00,  1.09it/s, loss=1.12, v_num=0, train_loss_step=1.110, train_acc_step=0.250, train_loss_epoch=1.110, train_acc_epoch=0.336]tcmalloc: large alloc 1707515904 bytes == 0x55a71c0e2000 @  0x7fc22b199615 0x55a6798424cc 0x55a67992247a 0x55a679848f0c 0x7fc2263eb9e4 0x7fc2263f3b14 0x7fc2263c8a60 0x7fc17dba6f55 0x7fc17dba288e 0x7fc17dbaa235 0x7fc2263c8fae 0x7fc225b3faa8 0x55a679846098 0x55a6798b94d9 0x55a6798b3ced 0x55a679846bda 0x55a6798b4915 0x55a6798b39ee 0x55a679846bda 0x55a6798b8d00 0x55a679846afa 0x55a6798b4915 0x55a6798b39ee 0x55a679846bda 0x55a6798b4c0d 0x55a679846afa 0x55a6798b4c0d 0x55a6798b39ee 0x55a679846bda 0x55a6798b4c0d 0x55a6798b39ee\n",
            "Epoch 2:  62% 380/610 [07:29<04:32,  1.18s/it, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  66% 400/610 [07:39<04:01,  1.15s/it, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  69% 420/610 [07:48<03:31,  1.11s/it, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  72% 440/610 [07:56<03:04,  1.08s/it, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  75% 460/610 [08:05<02:38,  1.06s/it, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  79% 480/610 [08:14<02:13,  1.03s/it, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  82% 500/610 [08:23<01:50,  1.01s/it, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  85% 520/610 [08:32<01:28,  1.02it/s, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  89% 540/610 [08:40<01:07,  1.04it/s, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  92% 560/610 [08:49<00:47,  1.06it/s, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  95% 580/610 [08:58<00:27,  1.08it/s, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Epoch 2:  98% 600/610 [09:07<00:09,  1.10it/s, loss=0.918, v_num=0, train_loss_step=0.848, train_acc_step=0.625, train_loss_epoch=1.100, train_acc_epoch=0.359]\n",
            "Validating:  98% 240/245 [01:46<00:02,  2.27it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.1372, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4367, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 610/610 [09:18<00:00,  1.09it/s, loss=0.93, v_num=0, train_loss_step=1.120, train_acc_step=0.500, train_loss_epoch=1.100, train_acc_epoch=0.359] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.9993, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.5010, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  62% 380/610 [07:29<04:31,  1.18s/it, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  66% 400/610 [07:38<04:00,  1.15s/it, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  69% 420/610 [07:47<03:31,  1.11s/it, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  72% 440/610 [07:56<03:03,  1.08s/it, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  75% 460/610 [08:04<02:38,  1.05s/it, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  79% 480/610 [08:13<02:13,  1.03s/it, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  82% 500/610 [08:22<01:50,  1.01s/it, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  85% 520/610 [08:31<01:28,  1.02it/s, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  89% 540/610 [08:40<01:07,  1.04it/s, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  92% 560/610 [08:48<00:47,  1.06it/s, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  95% 580/610 [08:57<00:27,  1.08it/s, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Epoch 3:  98% 600/610 [09:06<00:09,  1.10it/s, loss=0.612, v_num=0, train_loss_step=0.215, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "Validating:  98% 240/245 [01:46<00:02,  2.27it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.5503, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4510, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 610/610 [09:17<00:00,  1.09it/s, loss=0.565, v_num=0, train_loss_step=0.495, train_acc_step=0.875, train_loss_epoch=0.999, train_acc_epoch=0.501]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.7356, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.6928, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  62% 380/610 [07:29<04:32,  1.18s/it, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  66% 400/610 [07:39<04:01,  1.15s/it, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  69% 420/610 [07:48<03:31,  1.11s/it, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  72% 440/610 [07:56<03:04,  1.08s/it, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  75% 460/610 [08:05<02:38,  1.06s/it, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  79% 480/610 [08:14<02:13,  1.03s/it, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  82% 500/610 [08:23<01:50,  1.01s/it, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  85% 520/610 [08:32<01:28,  1.02it/s, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  89% 540/610 [08:40<01:07,  1.04it/s, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  92% 560/610 [08:49<00:47,  1.06it/s, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  95% 580/610 [08:58<00:27,  1.08it/s, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4:  98% 600/610 [09:07<00:09,  1.10it/s, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Epoch 4: 100% 610/610 [09:18<00:00,  1.09it/s, loss=0.395, v_num=0, train_loss_step=0.475, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.26it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.8113, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4592, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 610/610 [09:18<00:00,  1.09it/s, loss=0.465, v_num=0, train_loss_step=0.267, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.4669, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.8271, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 610/610 [09:27<00:00,  1.08it/s, loss=0.465, v_num=0, train_loss_step=0.267, train_acc_step=0.875, train_loss_epoch=0.736, train_acc_epoch=0.693]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRj6cMiqzgC3"
      },
      "source": [
        "train_easy_75_ambi_100 = pd.concat([train_df_easy.iloc[:2625], train_df_ambi])\n",
        "train_easy_75_ambi_100.to_csv('train_easy_75_ambi_100.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJu28MBt7Q49",
        "outputId": "806b7c57-0f8c-41a9-db1f-84d2177e22e7"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/train_easy_75_ambi_100.csv  --DEV_FILE /content/devnewdata_2.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/train_easy_75_ambi_100.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.1031, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.2500, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  66% 480/723 [09:53<05:00,  1.24s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  69% 500/723 [10:03<04:28,  1.21s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  72% 520/723 [10:11<03:58,  1.18s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  75% 540/723 [10:20<03:30,  1.15s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  77% 560/723 [10:29<03:03,  1.12s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  80% 580/723 [10:38<02:37,  1.10s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  83% 600/723 [10:47<02:12,  1.08s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  86% 620/723 [10:56<01:49,  1.06s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  89% 640/723 [11:05<01:26,  1.04s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  91% 660/723 [11:14<01:04,  1.02s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  94% 680/723 [11:23<00:43,  1.00s/it, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0:  97% 700/723 [11:31<00:22,  1.01it/s, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Epoch 0: 100% 720/723 [11:40<00:02,  1.03it/s, loss=0.888, v_num=1, train_loss_step=1.040, train_acc_step=0.625]\n",
            "Validating: 100% 245/245 [01:49<00:00,  2.26it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0435, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4791, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 723/723 [11:42<00:00,  1.03it/s, loss=1.01, v_num=1, train_loss_step=0.917, train_acc_step=0.500] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.0156, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.4699, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1092804608 bytes == 0x561be7bba000 @  0x7f0abfde1615 0x561acb9da4cc 0x561acbaba47a 0x561acb9e0f0c 0x7f0abb0339e4 0x7f0abb03bb14 0x7f0abb010a60 0x7f0a127eef55 0x7f0a127ea88e 0x7f0a127f2235 0x7f0abb010fae 0x7f0aba787aa8 0x561acb9de098 0x561acba514d9 0x561acba4bced 0x561acb9debda 0x561acba4c915 0x561acba4b9ee 0x561acb9debda 0x561acba50d00 0x561acb9deafa 0x561acba4c915 0x561acba4b9ee 0x561acb9debda 0x561acba4cc0d 0x561acb9deafa 0x561acba4cc0d 0x561acba4b9ee 0x561acb9debda 0x561acba4cc0d 0x561acba4b9ee\n",
            "tcmalloc: large alloc 1366007808 bytes == 0x561b6ec1e000 @  0x7f0abfde1615 0x561acb9da4cc 0x561acbaba47a 0x561acb9e0f0c 0x7f0abb0339e4 0x7f0abb03bb14 0x7f0abb010a60 0x7f0a127eef55 0x7f0a127ea88e 0x7f0a127f2235 0x7f0abb010fae 0x7f0aba787aa8 0x561acb9de098 0x561acba514d9 0x561acba4bced 0x561acb9debda 0x561acba4c915 0x561acba4b9ee 0x561acb9debda 0x561acba50d00 0x561acb9deafa 0x561acba4c915 0x561acba4b9ee 0x561acb9debda 0x561acba4cc0d 0x561acb9deafa 0x561acba4cc0d 0x561acba4b9ee 0x561acb9debda 0x561acba4cc0d 0x561acba4b9ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x561be7bba000 @  0x7f0abfde1615 0x561acb9da4cc 0x561acbaba47a 0x561acb9e0f0c 0x7f0abb0339e4 0x7f0abb03bb14 0x7f0abb010a60 0x7f0a127eef55 0x7f0a127ea88e 0x7f0a127f2235 0x7f0abb010fae 0x7f0aba787aa8 0x561acb9de098 0x561acba514d9 0x561acba4bced 0x561acb9debda 0x561acba4c915 0x561acba4b9ee 0x561acb9debda 0x561acba50d00 0x561acb9deafa 0x561acba4c915 0x561acba4b9ee 0x561acb9debda 0x561acba4cc0d 0x561acb9deafa 0x561acba4cc0d 0x561acba4b9ee 0x561acb9debda 0x561acba4cc0d 0x561acba4b9ee\n",
            "Epoch 1:  66% 480/723 [09:54<05:01,  1.24s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  69% 500/723 [10:04<04:29,  1.21s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  72% 520/723 [10:13<03:59,  1.18s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  75% 540/723 [10:22<03:30,  1.15s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  77% 560/723 [10:31<03:03,  1.13s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  80% 580/723 [10:39<02:37,  1.10s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  83% 600/723 [10:48<02:13,  1.08s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  86% 620/723 [10:57<01:49,  1.06s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  89% 640/723 [11:06<01:26,  1.04s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  91% 660/723 [11:15<01:04,  1.02s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  94% 680/723 [11:24<00:43,  1.01s/it, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1:  97% 700/723 [11:33<00:22,  1.01it/s, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Epoch 1: 100% 720/723 [11:42<00:02,  1.03it/s, loss=0.649, v_num=1, train_loss_step=0.296, train_acc_step=1.000, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "Validating: 100% 245/245 [01:49<00:00,  2.24it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.2111, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.5097, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 723/723 [11:44<00:00,  1.03it/s, loss=0.644, v_num=1, train_loss_step=0.627, train_acc_step=0.625, train_loss_epoch=1.020, train_acc_epoch=0.470]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.7136, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.7003, device='cuda:0')\n",
            "--------------------\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x561b6ec1e000 @  0x7f0abfde1615 0x561acb9da4cc 0x561acbaba47a 0x561acb9e0f0c 0x7f0abb0339e4 0x7f0abb03bb14 0x7f0abb010a60 0x7f0a127eef55 0x7f0a127ea88e 0x7f0a127f2235 0x7f0abb010fae 0x7f0aba787aa8 0x561acb9de098 0x561acba514d9 0x561acba4bced 0x561acb9debda 0x561acba4c915 0x561acba4b9ee 0x561acb9debda 0x561acba50d00 0x561acb9deafa 0x561acba4c915 0x561acba4b9ee 0x561acb9debda 0x561acba4cc0d 0x561acb9deafa 0x561acba4cc0d 0x561acba4b9ee 0x561acb9debda 0x561acba4cc0d 0x561acba4b9ee\n",
            "Epoch 2:  66% 480/723 [09:51<04:59,  1.23s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  69% 500/723 [10:00<04:28,  1.20s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  72% 520/723 [10:09<03:58,  1.17s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  75% 540/723 [10:18<03:29,  1.15s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  77% 560/723 [10:27<03:02,  1.12s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  80% 580/723 [10:36<02:36,  1.10s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  83% 600/723 [10:44<02:12,  1.07s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  86% 620/723 [10:53<01:48,  1.05s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  89% 640/723 [11:02<01:25,  1.04s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  91% 660/723 [11:11<01:04,  1.02s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  94% 680/723 [11:20<00:43,  1.00s/it, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2:  97% 700/723 [11:29<00:22,  1.02it/s, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Epoch 2: 100% 720/723 [11:37<00:02,  1.03it/s, loss=0.392, v_num=1, train_loss_step=0.701, train_acc_step=0.625, train_loss_epoch=0.714, train_acc_epoch=0.700]\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.9166, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.5087, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 723/723 [11:40<00:00,  1.03it/s, loss=0.47, v_num=1, train_loss_step=0.576, train_acc_step=0.750, train_loss_epoch=0.714, train_acc_epoch=0.700] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.3272, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.8794, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  66% 480/723 [09:45<04:56,  1.22s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  69% 500/723 [09:54<04:25,  1.19s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  72% 520/723 [10:03<03:55,  1.16s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  75% 540/723 [10:12<03:27,  1.13s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  77% 560/723 [10:21<03:00,  1.11s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  80% 580/723 [10:29<02:35,  1.09s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  83% 600/723 [10:38<02:10,  1.06s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  86% 620/723 [10:47<01:47,  1.04s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  89% 640/723 [10:56<01:25,  1.03s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  91% 660/723 [11:04<01:03,  1.01s/it, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  94% 680/723 [11:13<00:42,  1.01it/s, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3:  97% 700/723 [11:22<00:22,  1.03it/s, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Epoch 3: 100% 720/723 [11:31<00:02,  1.04it/s, loss=0.0651, v_num=1, train_loss_step=0.0168, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879]\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(3.0885, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.5199, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 723/723 [11:33<00:00,  1.04it/s, loss=0.152, v_num=1, train_loss_step=0.0807, train_acc_step=1.000, train_loss_epoch=0.327, train_acc_epoch=0.879] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.1303, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.9506, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  66% 480/723 [09:45<04:56,  1.22s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  69% 500/723 [09:54<04:25,  1.19s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  72% 520/723 [10:03<03:55,  1.16s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  75% 540/723 [10:12<03:27,  1.13s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  77% 560/723 [10:21<03:00,  1.11s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  80% 580/723 [10:30<02:35,  1.09s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  83% 600/723 [10:38<02:10,  1.06s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  86% 620/723 [10:47<01:47,  1.04s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  89% 640/723 [10:56<01:25,  1.03s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  91% 660/723 [11:05<01:03,  1.01s/it, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  94% 680/723 [11:13<00:42,  1.01it/s, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4:  97% 700/723 [11:22<00:22,  1.03it/s, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Epoch 4: 100% 720/723 [11:31<00:02,  1.04it/s, loss=0.0886, v_num=1, train_loss_step=0.000526, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(3.2868, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.5117, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 723/723 [11:33<00:00,  1.04it/s, loss=0.0355, v_num=1, train_loss_step=0.00642, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.0385, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.9874, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 723/723 [11:42<00:00,  1.03it/s, loss=0.0355, v_num=1, train_loss_step=0.00642, train_acc_step=1.000, train_loss_epoch=0.130, train_acc_epoch=0.951]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZDrcm097cVB"
      },
      "source": [
        "train_easy_50_ambi_50 = pd.concat([train_df_easy.iloc[:1750], train_df_ambi.iloc[:600]])\n",
        "train_easy_50_ambi_50.to_csv('train_easy_50_ambi_50.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5bGVK66Myzw",
        "outputId": "ea5c333f-a8ce-4c24-cdcb-5840d5c7b2d9"
      },
      "source": [
        "!python3 /content/mcq_training_lightning_1.py --model_name_or_path bert-base-uncased --hidden_dropout_prob 0.15 --max_input_seq_length 128 --output_dir ./  --predictions_file predictions.csv --TRAIN_FILE /content/train_easy_50_ambi_50.csv  --DEV_FILE /content/devnewdata_2.csv --train_batch_size 8 --eval_batch_size 8 --max_train_samples -1 --num_train_epochs 5 --gradient_accumulation_steps 1 --seed 42 --save_top_k -1 --learning_rate 5e-05 --write_dev_predictions"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)\n",
            "--------------------\n",
            "Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')\n",
            "--------------------\n",
            "Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/train_easy_50_ambi_50.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                  | Params\n",
            "------------------------------------------------\n",
            "0 | model | BertForMultipleChoice | 109 M \n",
            "------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.932   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------\n",
            "Validation avg_loss:  tensor(1.1031, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.2500, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Global seed set to 42\n",
            "Epoch 0:  56% 300/538 [05:59<04:45,  1.20s/it, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  59% 320/538 [06:08<04:11,  1.15s/it, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  63% 340/538 [06:17<03:39,  1.11s/it, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  67% 360/538 [06:26<03:11,  1.07s/it, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  71% 380/538 [06:35<02:44,  1.04s/it, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  74% 400/538 [06:43<02:19,  1.01s/it, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  78% 420/538 [06:52<01:55,  1.02it/s, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  82% 440/538 [07:01<01:33,  1.04it/s, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  86% 460/538 [07:10<01:12,  1.07it/s, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  89% 480/538 [07:18<00:53,  1.09it/s, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  93% 500/538 [07:27<00:34,  1.12it/s, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Epoch 0:  97% 520/538 [07:36<00:15,  1.14it/s, loss=1.05, v_num=2, train_loss_step=0.994, train_acc_step=0.500]\n",
            "Validating:  98% 240/245 [01:45<00:02,  2.28it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:47<00:00,  2.29it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.0459, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4515, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv\n",
            "--------------------\n",
            "Epoch 0: 100% 538/538 [07:47<00:00,  1.15it/s, loss=1.04, v_num=2, train_loss_step=0.899, train_acc_step=0.625]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(1.0788, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.4002, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 0: 100% 538/538 [07:48<00:00,  1.15it/s, loss=1.04, v_num=2, train_loss_step=0.899, train_acc_step=0.625]tcmalloc: large alloc 1092804608 bytes == 0x558a6d58a000 @  0x7f39df8e6615 0x558950e624cc 0x558950f4247a 0x558950e68f0c 0x7f39dab389e4 0x7f39dab40b14 0x7f39dab15a60 0x7f39322f3f55 0x7f39322ef88e 0x7f39322f7235 0x7f39dab15fae 0x7f39da28caa8 0x558950e66098 0x558950ed94d9 0x558950ed3ced 0x558950e66bda 0x558950ed4915 0x558950ed39ee 0x558950e66bda 0x558950ed8d00 0x558950e66afa 0x558950ed4915 0x558950ed39ee 0x558950e66bda 0x558950ed4c0d 0x558950e66afa 0x558950ed4c0d 0x558950ed39ee 0x558950e66bda 0x558950ed4c0d 0x558950ed39ee\n",
            "tcmalloc: large alloc 1366007808 bytes == 0x5589f45ee000 @  0x7f39df8e6615 0x558950e624cc 0x558950f4247a 0x558950e68f0c 0x7f39dab389e4 0x7f39dab40b14 0x7f39dab15a60 0x7f39322f3f55 0x7f39322ef88e 0x7f39322f7235 0x7f39dab15fae 0x7f39da28caa8 0x558950e66098 0x558950ed94d9 0x558950ed3ced 0x558950e66bda 0x558950ed4915 0x558950ed39ee 0x558950e66bda 0x558950ed8d00 0x558950e66afa 0x558950ed4915 0x558950ed39ee 0x558950e66bda 0x558950ed4c0d 0x558950e66afa 0x558950ed4c0d 0x558950ed39ee 0x558950e66bda 0x558950ed4c0d 0x558950ed39ee\n",
            "tcmalloc: large alloc 1707515904 bytes == 0x558a6d58a000 @  0x7f39df8e6615 0x558950e624cc 0x558950f4247a 0x558950e68f0c 0x7f39dab389e4 0x7f39dab40b14 0x7f39dab15a60 0x7f39322f3f55 0x7f39322ef88e 0x7f39322f7235 0x7f39dab15fae 0x7f39da28caa8 0x558950e66098 0x558950ed94d9 0x558950ed3ced 0x558950e66bda 0x558950ed4915 0x558950ed39ee 0x558950e66bda 0x558950ed8d00 0x558950e66afa 0x558950ed4915 0x558950ed39ee 0x558950e66bda 0x558950ed4c0d 0x558950e66afa 0x558950ed4c0d 0x558950ed39ee 0x558950e66bda 0x558950ed4c0d 0x558950ed39ee\n",
            "Epoch 1:  56% 300/538 [05:59<04:45,  1.20s/it, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  59% 320/538 [06:08<04:11,  1.15s/it, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  63% 340/538 [06:17<03:39,  1.11s/it, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  67% 360/538 [06:26<03:11,  1.07s/it, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  71% 380/538 [06:35<02:44,  1.04s/it, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  74% 400/538 [06:43<02:19,  1.01s/it, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  78% 420/538 [06:52<01:55,  1.02it/s, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  82% 440/538 [07:01<01:33,  1.04it/s, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  86% 460/538 [07:10<01:12,  1.07it/s, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  89% 480/538 [07:19<00:53,  1.09it/s, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  93% 500/538 [07:27<00:34,  1.12it/s, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Epoch 1:  97% 520/538 [07:36<00:15,  1.14it/s, loss=0.871, v_num=2, train_loss_step=0.663, train_acc_step=0.625, train_loss_epoch=1.080, train_acc_epoch=0.400]\n",
            "Validating:  98% 240/245 [01:46<00:02,  2.28it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.1632, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4638, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv\n",
            "--------------------\n",
            "Epoch 1: 100% 538/538 [07:47<00:00,  1.15it/s, loss=0.94, v_num=2, train_loss_step=1.470, train_acc_step=0.250, train_loss_epoch=1.080, train_acc_epoch=0.400] \n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.9085, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.5759, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 1: 100% 538/538 [07:51<00:00,  1.14it/s, loss=0.94, v_num=2, train_loss_step=1.470, train_acc_step=0.250, train_loss_epoch=1.080, train_acc_epoch=0.400]tcmalloc: large alloc 1707515904 bytes == 0x5589f45ee000 @  0x7f39df8e6615 0x558950e624cc 0x558950f4247a 0x558950e68f0c 0x7f39dab389e4 0x7f39dab40b14 0x7f39dab15a60 0x7f39322f3f55 0x7f39322ef88e 0x7f39322f7235 0x7f39dab15fae 0x7f39da28caa8 0x558950e66098 0x558950ed94d9 0x558950ed3ced 0x558950e66bda 0x558950ed4915 0x558950ed39ee 0x558950e66bda 0x558950ed8d00 0x558950e66afa 0x558950ed4915 0x558950ed39ee 0x558950e66bda 0x558950ed4c0d 0x558950e66afa 0x558950ed4c0d 0x558950ed39ee 0x558950e66bda 0x558950ed4c0d 0x558950ed39ee\n",
            "Epoch 2:  56% 300/538 [05:59<04:45,  1.20s/it, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  59% 320/538 [06:09<04:11,  1.15s/it, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  63% 340/538 [06:17<03:40,  1.11s/it, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  67% 360/538 [06:26<03:11,  1.07s/it, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  71% 380/538 [06:35<02:44,  1.04s/it, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  74% 400/538 [06:44<02:19,  1.01s/it, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  78% 420/538 [06:53<01:56,  1.02it/s, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  82% 440/538 [07:01<01:33,  1.04it/s, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  86% 460/538 [07:10<01:13,  1.07it/s, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  89% 480/538 [07:19<00:53,  1.09it/s, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  93% 500/538 [07:28<00:34,  1.12it/s, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Epoch 2:  97% 520/538 [07:36<00:15,  1.14it/s, loss=0.478, v_num=2, train_loss_step=0.474, train_acc_step=0.750, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "Validating:  98% 240/245 [01:46<00:02,  2.28it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(1.6306, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4724, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv\n",
            "--------------------\n",
            "Epoch 2: 100% 538/538 [07:48<00:00,  1.15it/s, loss=0.543, v_num=2, train_loss_step=0.815, train_acc_step=0.625, train_loss_epoch=0.908, train_acc_epoch=0.576]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.5170, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.7892, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 3:  56% 300/538 [05:59<04:44,  1.20s/it, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  59% 320/538 [06:08<04:11,  1.15s/it, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  63% 340/538 [06:17<03:39,  1.11s/it, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  67% 360/538 [06:26<03:10,  1.07s/it, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  71% 380/538 [06:34<02:44,  1.04s/it, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  74% 400/538 [06:43<02:19,  1.01s/it, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  78% 420/538 [06:52<01:55,  1.02it/s, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  82% 440/538 [07:01<01:33,  1.04it/s, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  86% 460/538 [07:10<01:12,  1.07it/s, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  89% 480/538 [07:18<00:53,  1.09it/s, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  93% 500/538 [07:27<00:34,  1.12it/s, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Epoch 3:  97% 520/538 [07:36<00:15,  1.14it/s, loss=0.208, v_num=2, train_loss_step=0.149, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "Validating:  98% 240/245 [01:46<00:02,  2.28it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(2.2687, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4796, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv\n",
            "--------------------\n",
            "Epoch 3: 100% 538/538 [07:47<00:00,  1.15it/s, loss=0.252, v_num=2, train_loss_step=0.0456, train_acc_step=1.000, train_loss_epoch=0.517, train_acc_epoch=0.789]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.2097, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.9262, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4:  56% 300/538 [05:59<04:45,  1.20s/it, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  59% 320/538 [06:09<04:11,  1.15s/it, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  63% 340/538 [06:17<03:40,  1.11s/it, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  67% 360/538 [06:26<03:11,  1.07s/it, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  71% 380/538 [06:35<02:44,  1.04s/it, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  74% 400/538 [06:44<02:19,  1.01s/it, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  78% 420/538 [06:52<01:56,  1.02it/s, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  82% 440/538 [07:01<01:33,  1.04it/s, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  86% 460/538 [07:10<01:12,  1.07it/s, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  89% 480/538 [07:19<00:53,  1.09it/s, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  93% 500/538 [07:28<00:34,  1.12it/s, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Epoch 4:  97% 520/538 [07:36<00:15,  1.14it/s, loss=0.0675, v_num=2, train_loss_step=0.0218, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "Validating:  98% 240/245 [01:46<00:02,  2.28it/s]\u001b[A\n",
            "Validating: 100% 245/245 [01:48<00:00,  2.27it/s]\u001b[A--------------------\n",
            "Validation avg_loss:  tensor(2.6024, device='cuda:0')\n",
            "Validation avg_acc:  tensor(0.4765, device='cuda:0')\n",
            "Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv\n",
            "--------------------\n",
            "Epoch 4: 100% 538/538 [07:47<00:00,  1.15it/s, loss=0.0711, v_num=2, train_loss_step=0.00789, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n",
            "                                                 \u001b[A--------------------\n",
            "Train avg_loss:  tensor(0.0951, device='cuda:0')\n",
            "Train avg_acc:  tensor(0.9633, device='cuda:0')\n",
            "--------------------\n",
            "Epoch 4: 100% 538/538 [07:56<00:00,  1.13it/s, loss=0.0711, v_num=2, train_loss_step=0.00789, train_acc_step=1.000, train_loss_epoch=0.210, train_acc_epoch=0.926]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfeHGDu_M2qw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}