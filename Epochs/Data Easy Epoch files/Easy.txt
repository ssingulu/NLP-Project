Training arguments Namespace(adam_epsilon=1e-08, distributed_backend=None, early_stop_callback=False, fp_16=False, max_grad_norm=1.0, n_gpu=-1, num_workers=8, opt_level='O1', warmup_steps=0, weight_decay=0.0)
--------------------
Model arguments Namespace(hidden_dropout_prob=0.15, max_input_seq_length=128, model_name_or_path='bert-base-uncased')
--------------------
Other arguments Namespace(DEV_FILE='/content/devnewdata_2.csv', TRAIN_FILE='/content/data_easy.csv', do_fast_dev_run=False, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, limit_train_batches=-1, limit_val_batches=-1, max_train_samples=-1, num_train_epochs=5, output_dir='./', predictions_file='predictions.csv', save_last=False, save_top_k=-1, seed=42, train_batch_size=8, write_dev_predictions=True)
--------------------
Global seed set to 42
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                  | Params
------------------------------------------------
0 | model | BertForMultipleChoice | 109 M 
------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.932   Total estimated model params size (MB)
/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /content exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Validation sanity check:   0% 0/2 [00:00<?, ?it/s]--------------------
Validation avg_loss:  tensor(1.1031, device='cuda:0')
Validation avg_acc:  tensor(0.2500, device='cuda:0')
Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv
--------------------
Global seed set to 42
Epoch 0:  65% 440/682 [08:55<04:54,  1.22s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Validating: 0it [00:00, ?it/s]
Validating:   0% 0/245 [00:00<?, ?it/s]
Epoch 0:  67% 460/682 [09:05<04:23,  1.19s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  70% 480/682 [09:14<03:53,  1.15s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  73% 500/682 [09:22<03:24,  1.13s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  76% 520/682 [09:31<02:58,  1.10s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  79% 540/682 [09:40<02:32,  1.08s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  82% 560/682 [09:49<02:08,  1.05s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  85% 580/682 [09:58<01:45,  1.03s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  88% 600/682 [10:07<01:22,  1.01s/it, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  91% 620/682 [10:16<01:01,  1.01it/s, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  94% 640/682 [10:24<00:41,  1.02it/s, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0:  97% 660/682 [10:33<00:21,  1.04it/s, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Epoch 0: 100% 680/682 [10:42<00:01,  1.06it/s, loss=0.677, v_num=2, train_loss_step=0.598, train_acc_step=0.750]
Validating: 100% 245/245 [01:48<00:00,  2.27it/s]--------------------
Validation avg_loss:  tensor(1.4240, device='cuda:0')
Validation avg_acc:  tensor(0.4842, device='cuda:0')
Writing predictions for /content/devnewdata_2.csv to ./epoch_0_predictions.csv
--------------------
Epoch 0: 100% 682/682 [10:44<00:00,  1.06it/s, loss=0.552, v_num=2, train_loss_step=0.523, train_acc_step=0.750]
                                                 --------------------
Train avg_loss:  tensor(0.7570, device='cuda:0')
Train avg_acc:  tensor(0.6676, device='cuda:0')
--------------------
tcmalloc: large alloc 1092804608 bytes == 0x5583128d4000 @  0x7fb034d22615 0x5581f6a9d4cc 0x5581f6b7d47a 0x5581f6aa3f0c 0x7fb02ff749e4 0x7fb02ff7cb14 0x7fb02ff51a60 0x7faf8772ff55 0x7faf8772b88e 0x7faf87733235 0x7fb02ff51fae 0x7fb02f6c8aa8 0x5581f6aa1098 0x5581f6b144d9 0x5581f6b0eced 0x5581f6aa1bda 0x5581f6b0f915 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b13d00 0x5581f6aa1afa 0x5581f6b0f915 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b0fc0d 0x5581f6aa1afa 0x5581f6b0fc0d 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b0fc0d 0x5581f6b0e9ee
tcmalloc: large alloc 1366007808 bytes == 0x558299938000 @  0x7fb034d22615 0x5581f6a9d4cc 0x5581f6b7d47a 0x5581f6aa3f0c 0x7fb02ff749e4 0x7fb02ff7cb14 0x7fb02ff51a60 0x7faf8772ff55 0x7faf8772b88e 0x7faf87733235 0x7fb02ff51fae 0x7fb02f6c8aa8 0x5581f6aa1098 0x5581f6b144d9 0x5581f6b0eced 0x5581f6aa1bda 0x5581f6b0f915 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b13d00 0x5581f6aa1afa 0x5581f6b0f915 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b0fc0d 0x5581f6aa1afa 0x5581f6b0fc0d 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b0fc0d 0x5581f6b0e9ee
tcmalloc: large alloc 1707515904 bytes == 0x5583128d4000 @  0x7fb034d22615 0x5581f6a9d4cc 0x5581f6b7d47a 0x5581f6aa3f0c 0x7fb02ff749e4 0x7fb02ff7cb14 0x7fb02ff51a60 0x7faf8772ff55 0x7faf8772b88e 0x7faf87733235 0x7fb02ff51fae 0x7fb02f6c8aa8 0x5581f6aa1098 0x5581f6b144d9 0x5581f6b0eced 0x5581f6aa1bda 0x5581f6b0f915 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b13d00 0x5581f6aa1afa 0x5581f6b0f915 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b0fc0d 0x5581f6aa1afa 0x5581f6b0fc0d 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b0fc0d 0x5581f6b0e9ee
Epoch 1:  65% 440/682 [08:56<04:54,  1.22s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Validating: 0it [00:00, ?it/s]
Validating:   0% 0/245 [00:00<?, ?it/s]
Epoch 1:  67% 460/682 [09:05<04:23,  1.19s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  70% 480/682 [09:14<03:53,  1.16s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  73% 500/682 [09:23<03:25,  1.13s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  76% 520/682 [09:32<02:58,  1.10s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  79% 540/682 [09:41<02:32,  1.08s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  82% 560/682 [09:49<02:08,  1.05s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  85% 580/682 [09:58<01:45,  1.03s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  88% 600/682 [10:07<01:23,  1.01s/it, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  91% 620/682 [10:16<01:01,  1.01it/s, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  94% 640/682 [10:25<00:41,  1.02it/s, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1:  97% 660/682 [10:34<00:21,  1.04it/s, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Epoch 1: 100% 680/682 [10:42<00:01,  1.06it/s, loss=0.264, v_num=2, train_loss_step=0.0316, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668]
Validating: 100% 245/245 [01:48<00:00,  2.26it/s]--------------------
Validation avg_loss:  tensor(2.4864, device='cuda:0')
Validation avg_acc:  tensor(0.4995, device='cuda:0')
Writing predictions for /content/devnewdata_2.csv to ./epoch_1_predictions.csv
--------------------
Epoch 1: 100% 682/682 [10:45<00:00,  1.06it/s, loss=0.427, v_num=2, train_loss_step=0.113, train_acc_step=1.000, train_loss_epoch=0.757, train_acc_epoch=0.668] 
                                                 --------------------
Train avg_loss:  tensor(0.3360, device='cuda:0')
Train avg_acc:  tensor(0.8741, device='cuda:0')
--------------------
tcmalloc: large alloc 1707515904 bytes == 0x558299938000 @  0x7fb034d22615 0x5581f6a9d4cc 0x5581f6b7d47a 0x5581f6aa3f0c 0x7fb02ff749e4 0x7fb02ff7cb14 0x7fb02ff51a60 0x7faf8772ff55 0x7faf8772b88e 0x7faf87733235 0x7fb02ff51fae 0x7fb02f6c8aa8 0x5581f6aa1098 0x5581f6b144d9 0x5581f6b0eced 0x5581f6aa1bda 0x5581f6b0f915 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b13d00 0x5581f6aa1afa 0x5581f6b0f915 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b0fc0d 0x5581f6aa1afa 0x5581f6b0fc0d 0x5581f6b0e9ee 0x5581f6aa1bda 0x5581f6b0fc0d 0x5581f6b0e9ee
Epoch 2:  65% 440/682 [08:56<04:55,  1.22s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Validating: 0it [00:00, ?it/s]
Validating:   0% 0/245 [00:00<?, ?it/s]
Epoch 2:  67% 460/682 [09:06<04:23,  1.19s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  70% 480/682 [09:15<03:53,  1.16s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  73% 500/682 [09:23<03:25,  1.13s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  76% 520/682 [09:32<02:58,  1.10s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  79% 540/682 [09:41<02:32,  1.08s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  82% 560/682 [09:50<02:08,  1.05s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  85% 580/682 [09:59<01:45,  1.03s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  88% 600/682 [10:08<01:23,  1.01s/it, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  91% 620/682 [10:16<01:01,  1.00it/s, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  94% 640/682 [10:25<00:41,  1.02it/s, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2:  97% 660/682 [10:34<00:21,  1.04it/s, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Epoch 2: 100% 680/682 [10:43<00:01,  1.06it/s, loss=0.102, v_num=2, train_loss_step=0.0949, train_acc_step=0.875, train_loss_epoch=0.336, train_acc_epoch=0.874]
Validating: 100% 245/245 [01:49<00:00,  2.25it/s]--------------------
Validation avg_loss:  tensor(3.5878, device='cuda:0')
Validation avg_acc:  tensor(0.5026, device='cuda:0')
Writing predictions for /content/devnewdata_2.csv to ./epoch_2_predictions.csv
--------------------
Epoch 2: 100% 682/682 [10:45<00:00,  1.06it/s, loss=0.238, v_num=2, train_loss_step=0.0135, train_acc_step=1.000, train_loss_epoch=0.336, train_acc_epoch=0.874]
                                                 --------------------
Train avg_loss:  tensor(0.1080, device='cuda:0')
Train avg_acc:  tensor(0.9631, device='cuda:0')
--------------------
Epoch 3:  65% 440/682 [08:57<04:55,  1.22s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Validating: 0it [00:00, ?it/s]
Validating:   0% 0/245 [00:00<?, ?it/s]
Epoch 3:  67% 460/682 [09:06<04:23,  1.19s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  70% 480/682 [09:15<03:53,  1.16s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  73% 500/682 [09:24<03:25,  1.13s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  76% 520/682 [09:33<02:58,  1.10s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  79% 540/682 [09:42<02:33,  1.08s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  82% 560/682 [09:50<02:08,  1.06s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  85% 580/682 [09:59<01:45,  1.03s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  88% 600/682 [10:08<01:23,  1.01s/it, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  91% 620/682 [10:17<01:01,  1.00it/s, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  94% 640/682 [10:26<00:41,  1.02it/s, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3:  97% 660/682 [10:35<00:21,  1.04it/s, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Epoch 3: 100% 680/682 [10:44<00:01,  1.06it/s, loss=0.0345, v_num=2, train_loss_step=0.000309, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963]
Validating: 100% 245/245 [01:49<00:00,  2.25it/s]--------------------
Validation avg_loss:  tensor(4.1857, device='cuda:0')
Validation avg_acc:  tensor(0.5112, device='cuda:0')
Writing predictions for /content/devnewdata_2.csv to ./epoch_3_predictions.csv
--------------------
Epoch 3: 100% 682/682 [10:46<00:00,  1.06it/s, loss=0.0128, v_num=2, train_loss_step=7.19e-5, train_acc_step=1.000, train_loss_epoch=0.108, train_acc_epoch=0.963] 
                                                 --------------------
Train avg_loss:  tensor(0.0286, device='cuda:0')
Train avg_acc:  tensor(0.9914, device='cuda:0')
--------------------
Epoch 4:  65% 440/682 [08:57<04:55,  1.22s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Validating: 0it [00:00, ?it/s]
Validating:   0% 0/245 [00:00<?, ?it/s]
Epoch 4:  67% 460/682 [09:07<04:24,  1.19s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  70% 480/682 [09:16<03:54,  1.16s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  73% 500/682 [09:25<03:25,  1.13s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  76% 520/682 [09:33<02:58,  1.10s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  79% 540/682 [09:42<02:33,  1.08s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  82% 560/682 [09:51<02:08,  1.06s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  85% 580/682 [10:00<01:45,  1.04s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  88% 600/682 [10:09<01:23,  1.02s/it, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  91% 620/682 [10:18<01:01,  1.00it/s, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  94% 640/682 [10:27<00:41,  1.02it/s, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4:  97% 660/682 [10:36<00:21,  1.04it/s, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Epoch 4: 100% 680/682 [10:44<00:01,  1.05it/s, loss=0.00459, v_num=2, train_loss_step=0.000112, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
Validating: 100% 245/245 [01:49<00:00,  2.24it/s]--------------------
Validation avg_loss:  tensor(4.1799, device='cuda:0')
Validation avg_acc:  tensor(0.5077, device='cuda:0')
Writing predictions for /content/devnewdata_2.csv to ./epoch_4_predictions.csv
--------------------
Epoch 4: 100% 682/682 [10:47<00:00,  1.05it/s, loss=0.000841, v_num=2, train_loss_step=0.000141, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]
                                                 --------------------
Train avg_loss:  tensor(0.0110, device='cuda:0')
Train avg_acc:  tensor(0.9971, device='cuda:0')
--------------------
Epoch 4: 100% 682/682 [10:55<00:00,  1.04it/s, loss=0.000841, v_num=2, train_loss_step=0.000141, train_acc_step=1.000, train_loss_epoch=0.0286, train_acc_epoch=0.991]